{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# data collection\n",
    "This notebook is responsible for collecting CrossFit 2018 Open Leaderboard data and athelete profile data *as represented at the time of data collection*."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## imports\n",
    "The below is just a set of import statements required to run the code in this notebook. A description for the purpose of each import statement should be commented above it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#sql connector\n",
    "import pymysql as pms\n",
    "#time recording and sleeping\n",
    "from time import time, sleep\n",
    "#browser automation\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## database credentials\n",
    "In order to connect to a local MySQL database, the block below runs to read the username, password, database name, and host required to establish a connection."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "db_user = \"\"\n",
    "db_pass = \"\"\n",
    "db_name = \"\"\n",
    "db_host = \"\"\n",
    "with open(\"database_credentials.txt\") as f:\n",
    "    db_user = f.readline().strip()\n",
    "    db_pass = f.readline().strip()\n",
    "    db_name = f.readline().strip()\n",
    "    db_host = f.readline().strip()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## test database connection\n",
    "This short snippet is going to attempt to connect to the database and drop out without doing anything. This is just to make sure the credentials and PyMySQL are working properly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_connect():\n",
    "    \"\"\"\n",
    "    Returns a database connection object using the default params\n",
    "    specified in the database_credentials file.\n",
    "    \"\"\"\n",
    "    return pms.connect(host=db_host, user=db_user, passwd=db_pass, db=db_name, charset=\"utf8\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Connected.\n",
      "Got database cursor. Can make queries within here.\n",
      "Connection closed.\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    con = get_connect()\n",
    "    print(\"Connected.\")\n",
    "    with con.cursor() as cur:\n",
    "        print(\"Got database cursor. Can make queries within here.\")\n",
    "finally:\n",
    "    if con:\n",
    "        con.close()\n",
    "        print(\"Connection closed.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## urls\n",
    "These urls/variables can be used to jump to pages where leaderboard data is available."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "default_url = \"https://games.crossfit.com/leaderboard/open/2018?division=1&region=0&scaled=0&sort=0&occupation=0&page=1\"\n",
    "custom_url = \"https://games.crossfit.com/leaderboard/open/2018?division={}&region={}&scaled={}&sort={}&occupation={}&page={}\"\n",
    "athlete_url = \"https://games.crossfit.com/athlete/{}\"\n",
    "\n",
    "#this map would be used to substitute values into the\n",
    "#custom url string at position \"division={}\" in place\n",
    "#of the \"{}\"\n",
    "map_division = {\n",
    "    \"men\": 1,\n",
    "    \"women\": 2,\n",
    "    \"team\": 11,\n",
    "    #men aged (35-39) inclusive\n",
    "    \"m35-39\": 18,\n",
    "    #women aged (35-39) inclusive\n",
    "    \"w35-39\": 19,\n",
    "    \"m40-44\": 12,\n",
    "    \"w40-44\": 13,\n",
    "    \"m45-49\": 3,\n",
    "    \"w45-49\": 4,\n",
    "    \"m50-54\": 5,\n",
    "    \"w50-54\": 6,\n",
    "    \"m55-59\": 7,\n",
    "    \"w55-59\": 8,\n",
    "    \"m60+\": 9,\n",
    "    \"w60+\": 10,\n",
    "    #boys aged (16-17) inclusive\n",
    "    \"b16-17\": 16,\n",
    "    #girls aged (16-17) inclusive\n",
    "    \"g16-17\": 17,\n",
    "    \"b14-15\": 14,\n",
    "    \"g14-15\": 15\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## getting custom url values\n",
    "Although the URL custom attributes can be harded, the more robust solution is to write a browser automation step, prior to the main data collection, that acquires the custom url attributes corresponding to each filter. This is done below.\n",
    "\n",
    "The goal is to obtain all of the maps like the one above in a more robust manner.\n",
    "\n",
    "Also, in the below step, it's important to note that the year and competition can also be used to filter results. However, at the current time, regional data is not available for 2018 (the open just finished). Furthermore, previous open leaderboard have different HTML structure (would require additional scraping code), and I really only care about 2018. Additionally, Rx'd/scaled, per-workout, occupation, and region are also available filtering criteria.\n",
    "\n",
    "**Rx'd/scaled and occupation**\n",
    "I don't care about these for the time being. This repo will only consider non-specific occupation and Rx'd athletes.\n",
    "\n",
    "**per-workout and region**\n",
    "I'll be able to do this filtering on my own (hypothetically). In order to do so, the region and per-workout scores will be scraped from the leaderboard and filtered on later in other notebooks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "HTML filters which will be used for filtering:\n",
      "['division', 'region']\n"
     ]
    }
   ],
   "source": [
    "filters = [\"division\", \"region\"]\n",
    "print(\"HTML filters which will be used for filtering:\\n{}\".format(filters))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below we're going to go to the leaderboard (autonomously) and scrape the IDs [CrossFit](https://games.crossfit.com/leaderboard/open/2018?division=1&region=0&scaled=0&sort=0&occupation=0&page=1) uses for divisions and regions. Although I don't need to use the same IDs, it can only help to use the same mappings. When we're talking about IDs here, I mean the numeric values that would be used to plug into the `\"{}\"` occurences in the `custom_url` string above.\n",
    "\n",
    "Here's the documentation I use for [Selenium](http://selenium-python.readthedocs.io/locating-elements.html)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\unknown\\AppData\\Local\\conda\\conda\\envs\\crossfit_open_2018\\lib\\site-packages\\pymysql\\cursors.py:103: Warning: (1050, \"Table 'division' already exists\")\n",
      "  return self._nextset(False)\n"
     ]
    }
   ],
   "source": [
    "#attempt database connect\n",
    "try:\n",
    "    con = get_connect()\n",
    "    with con.cursor() as cur:\n",
    "        #create division and region tables if they don't exist\n",
    "        sql = \"\"\"\n",
    "        CREATE TABLE IF NOT EXISTS region (\n",
    "            id INT PRIMARY KEY,\n",
    "            region VARCHAR(24) NOT NULL\n",
    "        );\n",
    "        CREATE TABLE IF NOT EXISTS division (\n",
    "            id INT PRIMARY KEY,\n",
    "            division VARCHAR(16) NOT NULL\n",
    "        );\n",
    "        \"\"\"\n",
    "        cur.execute(sql)\n",
    "        \n",
    "        #attempt to get regions and divisions\n",
    "        result_counts = [-1, -1]\n",
    "        sql = \"\"\"\n",
    "        SELECT * FROM {};\n",
    "        \"\"\"\n",
    "        for i in range(len(filters)):\n",
    "            cur.execute(sql.format(filters[i]))\n",
    "            result = cur.fetchall()\n",
    "            #store number of results\n",
    "            result_counts[i] = len(result)\n",
    "            #output results\n",
    "            #print(\"==== {} results({}) ====\".format(filters[i], result_counts[i]))\n",
    "            #print(\", \".join([col[0] for col in cur.description]))\n",
    "            for j in range(result_counts[i]):\n",
    "                break\n",
    "                #print(\"{}: {}\".format(j, result[j]))\n",
    "        \n",
    "        #if results for both are not empty, the values have\n",
    "        #already been scraped, so skip this\n",
    "        if result_counts[0] < 1 or result_counts[1] < 1:\n",
    "            #Store id, region/div pairs as tuples (id_0, region_0/div_0)\n",
    "            #the below entries will have 2 lists of such tuples, 1 for\n",
    "            #each filter_id\n",
    "            entries = []\n",
    "            #spin up browser\n",
    "            driver = webdriver.Chrome()\n",
    "            driver.get(default_url)\n",
    "            \n",
    "            #iterate over useful filters\n",
    "            for i in range(len(filters)):\n",
    "                #ids are formatted with control- as a prefix\n",
    "                dropdown = driver.find_element_by_id(\"control-\" + filters[i])\n",
    "                options = dropdown.find_elements_by_tag_name(\"option\")\n",
    "                #aggregate entries\n",
    "                #also, this is making 2 calls to o.get_attribute(\"value\"), and this\n",
    "                #could be done \"more efficiently\" without the list comprehension\n",
    "                entries.append([(int(o.get_attribute(\"value\")), o.get_attribute(\"innerText\"))\n",
    "                            for o in options if o.get_attribute(\"value\") != \"\"])\n",
    "                #print(entries[i])\n",
    "            \n",
    "            #close driver\n",
    "            driver.close()\n",
    "        \n",
    "            #write entries to file\n",
    "            sql = \"\"\"\n",
    "            INSERT INTO {}(id, {}) VALUES\n",
    "                {}\n",
    "            \"\"\"\n",
    "            for i in range(len(filters)):\n",
    "                cur.execute(sql.format(filters[i], filters[i],\n",
    "                                       \",\\n\".join([\"({}, '{}')\".format(e[0], e[1]) for e in entries[i]])))\n",
    "            #commit inserts\n",
    "            con.commit()\n",
    "finally:\n",
    "    if con:\n",
    "        con.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## checkout filter values\n",
    "Now that we've ensured the filters are in the database, let's grab them and create mappings. These will be necessary for the athlete table."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#this map will contain maps for each filter\n",
    "filter_maps = {}\n",
    "try:\n",
    "    con = get_connect()\n",
    "    with con.cursor() as cur:\n",
    "        sql = \"\"\"\n",
    "        SELECT * FROM {};\n",
    "        \"\"\"\n",
    "        for f in filters:\n",
    "            #add filter map\n",
    "            filter_maps[f] = {}\n",
    "            #get results\n",
    "            cur.execute(sql.format(f))\n",
    "            result = cur.fetchall()\n",
    "            #store results\n",
    "            for r in result:\n",
    "                #create mapping from div/region -> id or vice-versa (depending on scraping needs)\n",
    "                if f == \"division\":\n",
    "                    filter_maps[f][r[0]] = r[1]\n",
    "                elif f == \"region\":\n",
    "                    filter_maps[f][r[1]] = r[0]\n",
    "finally:\n",
    "    if con:\n",
    "        con.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#filter_maps"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## hard-coded values\n",
    "At the time of data collection, the 2018 CrossFit Open has ended. With this, we're making the assumption the total number of pages for each leaderboard will not change (this is not necessarily 100%, but I'm assuming it's very close to 100%). The leaderboard is structured in a way such that if a page is requested beyond the total number of leaderboard pages available for a specific filter, it redirects back to the first leaderboard page.\n",
    "\n",
    "Although there are different ways to handle this, in order to know when to stop scraping for a specific filter, we'll just scrape the index of the last available page, which is available at the bottom of each leaderboard. Additionally, the current index for each filter will also scraped, defaulting to the first leaderboard page: 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\unknown\\AppData\\Local\\conda\\conda\\envs\\crossfit_open_2018\\lib\\site-packages\\pymysql\\cursors.py:166: Warning: (1050, \"Table 'worldwide_division_pages' already exists\")\n",
      "  result = self._query(query)\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    con = get_connect()\n",
    "    with con.cursor() as cur:\n",
    "        #create database to store these worldwide-division indices\n",
    "        #(could easily be stored in a flat file instead)\n",
    "        sql = \"\"\"\n",
    "        CREATE TABLE IF NOT EXISTS worldwide_division_pages (\n",
    "            division_id INT PRIMARY KEY,\n",
    "            FOREIGN KEY (division_id)\n",
    "                REFERENCES division(id)\n",
    "                ON DELETE CASCADE,\n",
    "            curr_page INT NOT NULL DEFAULT 1,\n",
    "            last_page INT NOT NULL DEFAULT -1\n",
    "        )\n",
    "        \"\"\"\n",
    "        cur.execute(sql)\n",
    "        \n",
    "        #check if table has been populated already in a previous run\n",
    "        sql = \"\"\"\n",
    "        SELECT * FROM worldwide_division_pages;\n",
    "        \"\"\"\n",
    "        cur.execute(sql)\n",
    "        result = cur.fetchall()\n",
    "        if len(result) == 0:\n",
    "            #cross-populate division id's from division table\n",
    "            sql = \"\"\"\n",
    "            INSERT INTO worldwide_division_pages (division_id)\n",
    "                SELECT id FROM division;\n",
    "            \"\"\"\n",
    "            cur.execute(sql)\n",
    "            con.commit()\n",
    "            \n",
    "            #grab option IDs from database\n",
    "            #this could be done by just collecting them from the browser, the values\n",
    "            #are supposed to be identical\n",
    "            sql = \"\"\"\n",
    "            SELECT division_id FROM worldwide_division_pages;\n",
    "            \"\"\"\n",
    "            cur.execute(sql)\n",
    "            result = cur.fetchall()\n",
    "            ids = [r[0] for r in result]\n",
    "            print(ids)\n",
    "                     \n",
    "            #scrape last_page values for each division\n",
    "            driver = webdriver.Chrome()\n",
    "            driver.get(default_url)\n",
    "            \n",
    "            #grab division selectable and store in browser\n",
    "            inject_store_select = \"\"\"\n",
    "            window.division_select = document.getElementById(\"control-division\");\n",
    "            \"\"\"\n",
    "            driver.execute_script(inject_store_select)\n",
    "            \n",
    "            last_pages = []\n",
    "            #iterate over ids\n",
    "            for i in ids:\n",
    "                #force select element to change to new dropdown option\n",
    "                inject_change_select = \"\"\"\n",
    "                window.division_select.value = {};\n",
    "                window.division_select.dispatchEvent(new Event(\"change\"));\n",
    "                \"\"\".format(i)\n",
    "                driver.execute_script(inject_change_select)\n",
    "                #wait for page to update\n",
    "                sleep(2)\n",
    "                #grab last page text from the bottom\n",
    "                last_pages.append(int(\n",
    "                    driver.find_element_by_class_name(\"nums\")\n",
    "                        .find_elements_by_tag_name(\"a\")[-1]\n",
    "                        .get_attribute(\"innerText\")\n",
    "                ))\n",
    "            \n",
    "            #write updates to file in bulk\n",
    "            sql = \"\"\"\n",
    "            INSERT INTO worldwide_division_pages(division_id, last_page) VALUES\n",
    "                {}\n",
    "                ON DUPLICATE KEY UPDATE last_page = VALUES(last_page);\n",
    "            \"\"\"\n",
    "            cur.execute(\n",
    "                sql.format(\n",
    "                    \",\\n\".join([\"({}, '{}')\".format(ids[i], last_pages[i]) for i in range(len(ids))])\n",
    "                )\n",
    "            )\n",
    "            con.commit()\n",
    "            \n",
    "            #close driver\n",
    "            driver.close()\n",
    "finally:\n",
    "    if con:\n",
    "        con.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### additional fixed filters\n",
    "Below are the hard-coded values for scaled, occupation, sort, and workout type."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "custom url:\n",
      "https://games.crossfit.com/leaderboard/open/2018?division={}&region=0&scaled=0&sort=0&occupation=0&page={}\n"
     ]
    }
   ],
   "source": [
    "region = 0\n",
    "scaled = 0\n",
    "sort = 0\n",
    "occupation = 0\n",
    "custom_url = (\n",
    "    \"https://games.crossfit.com/leaderboard/open/2018?division={}&region={}&scaled={}&sort={}&occupation={}&page={}\"\n",
    "        .format(\"{}\", region, scaled, sort, occupation, \"{}\")\n",
    ")\n",
    "print(\"custom url:\\n{}\".format(custom_url))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## leaderboard/athlete scraping overview\n",
    "Scraping the Open leaderboard data is only half of the data required per athlete. The remaining data will be collected from their athlete profile, containing statistics for their Back Squat, Fran, and other CrossFit staples. This will be done **after** the Open data for a specific division is completely finished. Therefore, the scraping process pipeline from this point forward is as follows:\n",
    "* for each division:\n",
    "    * for each leaderboard page:\n",
    "        * scrape all athlete leaderboard information\n",
    "        * for each athlete:\n",
    "            * scrape profile data\n",
    "        * write all athlete data to file\n",
    "        * update current page for this division"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### reading in the pages to be scraped from the Open leaderboard\n",
    "Below the pages left to be scraped will be collected from the database and sorted in order from least athletes to most athletes **per division**. Divisions which have complete Open leaderboard data already scraped will have a current page value exceeding the last page value by 1 (`curr_page = 3`, `last_page = 2`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "try:\n",
    "    con = get_connect()\n",
    "    with con.cursor() as cur:\n",
    "        sql = \"\"\"\n",
    "        SELECT * FROM worldwide_division_pages;\n",
    "        \"\"\"\n",
    "        cur.execute(sql)\n",
    "        #sort results based on last_page value\n",
    "        division_pages = sorted(\n",
    "            list(\n",
    "                map(\n",
    "                    lambda tup: list(tup),\n",
    "                    cur.fetchall()\n",
    "                )\n",
    "            ), \n",
    "            key=lambda tup: tup[2],\n",
    "            reverse=True\n",
    "        )\n",
    "finally:\n",
    "    if con:\n",
    "        con.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### filtering division pages\n",
    "Divisions that have already been completely scraped (`curr_page == last_page + 1`) should be excluded from the scraping process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original division pages:\n",
      "\t[1, 687, 4552]\n",
      "\t[2, 44, 3441]\n",
      "\t[18, 47, 894]\n",
      "\t[19, 59, 619]\n",
      "\t[12, 65, 568]\n",
      "\t[13, 47, 398]\n",
      "\t[3, 56, 337]\n",
      "\t[4, 45, 239]\n",
      "\t[5, 41, 167]\n",
      "\t[6, 54, 124]\n",
      "\t[7, 48, 91]\n",
      "\t[8, 61, 76]\n",
      "\t[16, 55, 63]\n",
      "\t[9, 51, 55]\n",
      "\t[17, 48, 47]\n",
      "\t[10, 40, 45]\n",
      "\t[14, 41, 40]\n",
      "\t[15, 33, 32]\n",
      "Filtered division pages:\n",
      "\t[1, 687, 4552]\n",
      "\t[2, 44, 3441]\n",
      "\t[18, 47, 894]\n",
      "\t[19, 59, 619]\n",
      "\t[12, 65, 568]\n",
      "\t[13, 47, 398]\n",
      "\t[3, 56, 337]\n",
      "\t[4, 45, 239]\n",
      "\t[5, 41, 167]\n",
      "\t[6, 54, 124]\n",
      "\t[7, 48, 91]\n",
      "\t[8, 61, 76]\n",
      "\t[16, 55, 63]\n",
      "\t[9, 51, 55]\n",
      "\t[10, 40, 45]\n"
     ]
    }
   ],
   "source": [
    "#output\n",
    "print(\"Original division pages:\\n{}\".format(\"\\n\".join([\"\\t{}\".format(d) for d in division_pages])))\n",
    "#filter\n",
    "filtered_division_pages = [d for d in division_pages if d[1] <= d[2]]\n",
    "print(\"Filtered division pages:\\n{}\".format(\"\\n\".join([\"\\t{}\".format(d) for d in filtered_division_pages])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## setting up the athlete tables\n",
    "The athlete table will contain data for athletes from 2 sources: leaderboards and profiles. The data from each in it's native form is shown below.\n",
    "\n",
    "### leaderboards\n",
    "<img src=\"images/leaderboard.png\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### profiles\n",
    "<img src=\"images/basic_stats.png\" />\n",
    "<img src=\"images/benchmark_stats.png\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Although this data could be separated into 2 or 3 different tables, for the scope of the planned analysis, it can all go into the same table. The collected data per athlete will contain the following:\n",
    "* leaderboard\n",
    "    * name\n",
    "    * 18.1, 18.2, 18.2a, 18.3, 18.4, 18.5 scores (not rank, a.k.a. time or reps or weight)\n",
    "    * height, weight, age\n",
    "    * region, division\n",
    "* profile\n",
    "    * affiliate\n",
    "    * back squat, clean and jerk, snatch, deadlift\n",
    "    * fight gone bad, fran, grace, helen, filthy 50\n",
    "    * max pull-ups\n",
    "    * sprint 400m, run 5k\n",
    "    \n",
    "The table containing all this data is created below. Time data will be recorded in seconds, weight in pounds, height in inches, and age in years."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\unknown\\AppData\\Local\\conda\\conda\\envs\\crossfit_open_2018\\lib\\site-packages\\pymysql\\cursors.py:166: Warning: (1050, \"Table 'athlete' already exists\")\n",
      "  result = self._query(query)\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    con = get_connect()\n",
    "    with con.cursor() as cur:\n",
    "        sql = \"\"\"\n",
    "        CREATE TABLE IF NOT EXISTS athlete (\n",
    "            id INT PRIMARY KEY,\n",
    "            name VARCHAR(128) NOT NULL,\n",
    "            leaderboard_18_1_reps INT,\n",
    "            leaderboard_18_2_time_secs INT,\n",
    "            leaderboard_18_2a_weight_lbs INT,\n",
    "            leaderboard_18_3_reps INT,\n",
    "            leaderboard_18_4_time_secs INT,\n",
    "            leaderboard_18_5_reps INT,\n",
    "            \n",
    "            height_in INT,\n",
    "            weight_lbs INT,\n",
    "            age_years INT,\n",
    "            \n",
    "            region_id INT NOT NULL,\n",
    "            FOREIGN KEY (region_id)\n",
    "                REFERENCES region(id)\n",
    "                ON DELETE CASCADE,\n",
    "            division_id INT NOT NULL,\n",
    "            FOREIGN KEY (division_id)\n",
    "                REFERENCES division(id)\n",
    "                ON DELETE CASCADE,\n",
    "            \n",
    "            affiliate_id INT,\n",
    "            \n",
    "            back_squat_lbs INT,\n",
    "            clean_and_jerk_lbs INT,\n",
    "            snatch_lbs INT,\n",
    "            deadlift_lbs INT,\n",
    "            \n",
    "            fight_gone_bad_time_secs INT,\n",
    "            fran_time_secs INT,\n",
    "            grace_time_secs INT,\n",
    "            helen_time_secs INT,\n",
    "            filthy_50_time_secs INT,\n",
    "            \n",
    "            max_pull_ups INT,\n",
    "            \n",
    "            sprint_400_m_time_secs INT,\n",
    "            run_5_km_time_secs INT\n",
    "        );\n",
    "        \"\"\"\n",
    "        cur.execute(sql)\n",
    "finally:\n",
    "    if con:\n",
    "        con.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## scrape athlete data\n",
    "Here we go. glhf\n",
    "\n",
    "### Constants used during scraping\n",
    "The below constants are used during the scraping process. The uses are commented, but they range from CSS selectors, output formatting, SQL column names, and time multipliers (h:m:s)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#used for output styling\n",
    "center_space = 60\n",
    "center_sep = \" \"\n",
    "\n",
    "#for leaderboard workouts\n",
    "workout_keys = [\n",
    "    \"18_1_reps\", \"18_2_time_secs\", \"18_2a_weight_lbs\", \"18_3_reps\", \"18_4_time_secs\", \"18_5_reps\"]\n",
    "workout_keys = list(map(lambda k: \"leaderboard_\" + k, workout_keys))\n",
    "\n",
    "#for stats metrics on the athlete profile page\n",
    "stats_keys = [\n",
    "    \"back_squat_lbs\",\n",
    "    \"clean_and_jerk_lbs\",\n",
    "    \"snatch_lbs\",\n",
    "    \n",
    "    \"deadlift_lbs\",\n",
    "    \"fight_gone_bad_time_secs\",\n",
    "    \"max_pull_ups\",\n",
    "    \n",
    "    \"fran_time_secs\",\n",
    "    \"grace_time_secs\",\n",
    "    \"helen_time_secs\",\n",
    "    \n",
    "    \"filthy_50_time_secs\",\n",
    "    \"sprint_400_m_time_secs\",\n",
    "    \"run_5_km_time_secs\"\n",
    "]\n",
    "\n",
    "#for other values (height, weight, etc.)\n",
    "height_key = \"height_in\"\n",
    "weight_key = \"weight_lbs\"\n",
    "age_key = \"age_years\"\n",
    "id_key = \"id\"\n",
    "region_key = \"region_id\"\n",
    "division_key = \"division_id\"\n",
    "name_key = \"name\"\n",
    "affiliate_key = \"affiliate_id\"\n",
    "\n",
    "#all keys (used for insertion)\n",
    "all_keys = sorted(workout_keys + stats_keys + [\n",
    "    height_key, weight_key, age_key, id_key, region_key, division_key, name_key, affiliate_key\n",
    "])\n",
    "all_keys_str = \", \".join(all_keys)\n",
    "\n",
    "#time map values for converting to seconds\n",
    "time_mults = [3600, 60, 1]\n",
    "\n",
    "#used for getting statistics on athlete profile page\n",
    "custom_stat_selects = [\n",
    "    \"li:nth-child({})\",\n",
    "    \"> .stats-section:nth-child({}) \",\n",
    "    \"> table > tbody > tr:nth-child({}) > td\"\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reformatting data\n",
    "The below funciton is used to reformat a lb/kg, reps, or hours:minutes:seconds value into it's database-equivalent format."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def handle_crossfit_score(html):\n",
    "    \"\"\"\n",
    "    Handles the parsing of several different CrossFit scores such as\n",
    "    weight (lb), time (x:y:z), reps (reps)/(), or none (--). Returns the\n",
    "    vale parsed to it's database required equivalent.\n",
    "    \"\"\"\n",
    "    #reps\n",
    "    if html[-4:] == \"reps\":\n",
    "        return int(html[:-5])\n",
    "    #time\n",
    "    elif \":\" in html:\n",
    "        hrs_mins_secs = list(map(lambda x: int(x), html.split(\":\")))\n",
    "        len_diff = len(time_mults) - len(hrs_mins_secs)\n",
    "        #collect all seconds\n",
    "        return sum(\n",
    "            [hrs_mins_secs[i] * time_mults[i + len_diff]\n",
    "                 for i in range(len(hrs_mins_secs))]\n",
    "        )\n",
    "    #no score\n",
    "    elif html == \"--\":\n",
    "        return -1\n",
    "    #weight\n",
    "    elif html[-2:] == \"lb\" or html[-2:] == \"kg\":\n",
    "        weight = int(html[:-3])\n",
    "        return weight if html[-2:] == \"lb\" else int(round(weight * 2.2))\n",
    "    #reps with no reps on the end (used for max pull-ups in athlete profile)\n",
    "    else:\n",
    "        #handle special values with -2 entry (200 reps for pullups is not happening in 2018)\n",
    "        parsed_val = int(html)\n",
    "        return parsed_val if parsed_val < 150 else -2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Random\n",
    "The below code has been refactored several times. However, the current implementation forces the divisions to be scraped at random. Once a division's leaderboard has been completely collected, the division is removed as a *required* scraping target (deleted from the local copy of division pages). This means that on iteration 17, the next 50 athletes from the Men's division could be scraped, and on iteration 18 Girl's (14-15) could be the target."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from random import randint"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The second codeblock here is the legacy version of the first codeblock. The legacy version completely scrapes a single division until it's completely collected, and then moves onto the next. The newer version randomly samples from the remaining incompletely scraped division on each iteration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                      Division Women 44/3441\n",
      "Page 44\n",
      "                               Division Women (40-44) 47/398\n",
      "Page 47\n",
      "                                 Division Men (40-44) 65/568\n",
      "Page 65\n",
      "                               Division Women (40-44) 48/398\n",
      "Page 48\n",
      "                                  Division Women (60+) 40/45\n",
      "Page 40\n",
      "                               Division Women (35-39) 59/619\n",
      "Page 59\n",
      "                                 Division Men (45-49) 56/337\n",
      "Page 56\n",
      "                               Division Women (40-44) 49/398\n",
      "Page 49\n",
      "                                      Division Women 45/3441\n",
      "Page 45\n",
      "                                  Division Men (55-59) 48/91\n",
      "Page 48\n",
      "                                 Division Men (40-44) 66/568\n",
      "Page 66\n"
     ]
    }
   ],
   "source": [
    "#spinup driver\n",
    "driver = webdriver.Chrome()\n",
    "\n",
    "#for each division\n",
    "#for div in division_pages:\n",
    "while 0 < len(filtered_division_pages):\n",
    "    #select division randomly\n",
    "    div = filtered_division_pages[randint(0, len(filtered_division_pages) - 1)]\n",
    "    \n",
    "    #output division\n",
    "    print(\"Division {} {}/{}\".format(filter_maps[\"division\"][div[0]], div[1], div[2])\n",
    "             .rjust(center_space, center_sep))\n",
    "    print(\"Page {}\".format(div[1]))\n",
    "\n",
    "    #go to page\n",
    "    driver.get(custom_url.format(div[0], div[1]))\n",
    "\n",
    "    #get leaderboard (might have to wait)\n",
    "    lb = WebDriverWait(driver, 5).until(\n",
    "        EC.presence_of_element_located(\n",
    "            (\n",
    "                By.CSS_SELECTOR,\n",
    "                \"body > #containerOverlay > #leaderboard > .lb-main > .inner > table > tbody\"\n",
    "            )\n",
    "        )\n",
    "    )\n",
    "    #get rows\n",
    "    rows = lb.find_elements_by_xpath(\"*\")\n",
    "    #collect athlete data\n",
    "    athletes = []\n",
    "    for r in rows:\n",
    "        #get columns and create empty dictionary\n",
    "        cols = r.find_elements_by_xpath(\"*\")#only td elements\n",
    "        dic = {}\n",
    "\n",
    "        #column 1\n",
    "        #name\n",
    "        names = cols[1].find_elements_by_css_selector(\"div > div > div:nth-child(2) > div\")\n",
    "        dic[name_key] = (\n",
    "            \"'\" + (\n",
    "                names[0].get_attribute(\"innerText\") + \" \" + names[1].get_attribute(\"innerText\")\n",
    "            ).replace(\"'\", \"\\\\'\") + \"'\"\n",
    "        )\n",
    "        #info = cols[1].find_element_by_class_name(\"info\")\n",
    "        info = cols[1].find_element_by_css_selector(\"div > .bottom > ul\")\n",
    "        info_lis = info.find_elements_by_css_selector(\"li\")\n",
    "        #id (profile identifier for url)\n",
    "        dic[id_key] = int(info.find_element_by_tag_name(\"a\").get_attribute(\"href\").split(\"/\")[-1])\n",
    "        #region/division ids\n",
    "        dic[region_key] = filter_maps[\"region\"][info_lis[0].get_attribute(\"innerText\")]\n",
    "        dic[division_key] = div[0]\n",
    "        #age\n",
    "        dic[age_key] = int(info_lis[1].get_attribute(\"innerText\").split(\" \")[1])\n",
    "        #height/weight (not mandatory fields)\n",
    "        if 2 < len(info_lis):\n",
    "            height_weight = info_lis[2].get_attribute(\"innerText\").split(\" \")\n",
    "            #height\n",
    "            if \"in\" in height_weight:\n",
    "                dic[height_key] = float(height_weight[height_weight.index(\"in\") - 1])\n",
    "            elif \"cm\" in height_weight:\n",
    "                dic[height_key] = float(height_weight[height_weight.index(\"cm\") - 1]) / 2.54\n",
    "            else:\n",
    "                dic[height_key] = -1\n",
    "            dic[height_key] = int(round(dic[height_key]))\n",
    "            #weight\n",
    "            if \"lb\" in height_weight:\n",
    "                dic[weight_key] = float(height_weight[height_weight.index(\"lb\") - 1])\n",
    "            elif \"kg\" in height_weight:\n",
    "                dic[weight_key] = float(height_weight[height_weight.index(\"kg\") - 1]) * 2.20462\n",
    "            else:\n",
    "                dic[weight_key] = -1\n",
    "            dic[weight_key] = int(round(dic[weight_key]))\n",
    "\n",
    "        #columns 3-8 (inclusive, 0-indexed)\n",
    "        #leaderboard workout reps, time, or weight\n",
    "        scaled = False\n",
    "        first_col = 3\n",
    "        for i in range(first_col, 9):\n",
    "            #get inner value minus the surrounding parentheses\n",
    "            html_raw = cols[i].find_element_by_css_selector(\n",
    "                \"div > div > span > span:nth-child(2)\"\n",
    "            ).get_attribute(\"innerText\")\n",
    "            html = html_raw[html_raw.index(\"(\") + 1:-1]\n",
    "            #print(html_raw, html)\n",
    "            #if athlete scaled a workout, don't any data\n",
    "            if html.endswith(\"- s\"):\n",
    "                scaled = True\n",
    "                break\n",
    "            #convert values\n",
    "            key = workout_keys[i - first_col]\n",
    "            #handle html\n",
    "            dic[key] = handle_crossfit_score(html)\n",
    "\n",
    "        #append athlete if not scaled\n",
    "        if not scaled:\n",
    "            athletes.append(dic)\n",
    "    #iterate over athletes\n",
    "    for a in athletes:\n",
    "        driver.get(athlete_url.format(a[\"id\"]))\n",
    "        #wait for bottom stats to laod\n",
    "        #get leaderboard (might have to wait)\n",
    "        try:\n",
    "            #get stats\n",
    "            stats = WebDriverWait(driver, 1).until(\n",
    "                EC.presence_of_element_located(\n",
    "                    (\n",
    "                        By.CSS_SELECTOR,\n",
    "                        \"body #athleteProfile > div:nth-last-child(3) > .container > .stats-container\"\n",
    "                    )\n",
    "                )\n",
    "            )\n",
    "            #iterate and collect numbers\n",
    "            for i in range(1, 3):\n",
    "                sel0 = custom_stat_selects[0].format(i)\n",
    "                for j in range(1, 3):\n",
    "                    sel1 = custom_stat_selects[1].format(j)\n",
    "                    for k in range(1, 4):\n",
    "                        sel2 = custom_stat_selects[2].format(k)\n",
    "                        #select\n",
    "                        html = stats.find_element_by_css_selector(sel0 + sel1 + sel2).get_attribute(\"innerText\")\n",
    "                        a[stats_keys[6 * (i - 1) + 3 * (j - 1) + (k - 1)]] = handle_crossfit_score(html)\n",
    "        except:\n",
    "            #pass\n",
    "            for k in stats_keys:\n",
    "                a[k] = -1\n",
    "\n",
    "        #get affiliate id\n",
    "        try:\n",
    "            a[affiliate_key] = int(\n",
    "                (\n",
    "                    driver.find_element_by_css_selector(\n",
    "                        \"body #athleteProfile > .page-cover .bg-games-black-overlay .infobar > li:nth-child(6) a\"\n",
    "                    ).get_attribute(\"href\")\n",
    "                ).split(\"/\")[-1]\n",
    "            )\n",
    "        except:\n",
    "            a[affiliate_key] = -1\n",
    "\n",
    "    #\"stats-container\" class may not be present on profile pages (bs, dl, cj, ...)\n",
    "    #for a in athletes:\n",
    "    #    print(\"\\n\".join([\"{}: {}\".format(k, a[k]) for k in sorted(list(a.keys()))]) + \"\\n\")\n",
    "\n",
    "    #update database\n",
    "    try:\n",
    "        con = get_connect()\n",
    "        with con.cursor() as cur:\n",
    "            #insert athletes (if any from this page did not scale)\n",
    "            if len(athletes) != 0:\n",
    "                sql = \"\"\"\n",
    "                INSERT INTO athlete ({}) VALUES\n",
    "                    {}\n",
    "                    ON DUPLICATE KEY UPDATE id={};\n",
    "                \"\"\".format(\n",
    "                    all_keys_str,\n",
    "                    \",\\n\".join([\"(\" + \",\".join([str(a[k]) for k in all_keys]) + \")\" for a in athletes]),\n",
    "                    id_key\n",
    "                )\n",
    "                cur.execute(sql)\n",
    "                con.commit()\n",
    "\n",
    "            #increase value of current page\n",
    "            sql = \"\"\"\n",
    "            UPDATE worldwide_division_pages SET curr_page = {} WHERE division_id = {};\n",
    "            \"\"\".format(\n",
    "                div[1] + 1,\n",
    "                div[0]\n",
    "            )\n",
    "            cur.execute(sql)\n",
    "            con.commit()\n",
    "    finally:\n",
    "        if con:\n",
    "            con.close()\n",
    "\n",
    "    #increase local value of current page\n",
    "    div[1] += 1\n",
    "    #remove division if current page exceeds last page (after increment)\n",
    "    if div[1] == div[2] + 1:\n",
    "        filtered_division_pages.remove(div)\n",
    "\n",
    "#close browser\n",
    "driver.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "scrolled": true
   },
   "source": [
    "#spinup driver\n",
    "driver = webdriver.Chrome()\n",
    "\n",
    "#for each division\n",
    "for div in division_pages:\n",
    "    #output division\n",
    "    print(\"Division {}\".format(filter_maps[\"division\"][div[0]])\n",
    "             .rjust(center_space, center_sep))\n",
    "    #continue until current page exceeds last page\n",
    "    while div[1] <= div[2]:\n",
    "        print(\"Page {}\".format(div[1]))\n",
    "        \n",
    "        #go to page\n",
    "        driver.get(custom_url.format(div[0], div[1]))\n",
    "        \n",
    "        #get leaderboard (might have to wait)\n",
    "        lb = WebDriverWait(driver, 5).until(\n",
    "            EC.presence_of_element_located(\n",
    "                (\n",
    "                    By.CSS_SELECTOR,\n",
    "                    \"body > #containerOverlay > #leaderboard > .lb-main > .inner > table > tbody\"\n",
    "                )\n",
    "            )\n",
    "        )\n",
    "        #get rows\n",
    "        rows = lb.find_elements_by_xpath(\"*\")\n",
    "        #collect athlete data\n",
    "        athletes = []\n",
    "        for r in rows:\n",
    "            #get columns and create empty dictionary\n",
    "            cols = r.find_elements_by_xpath(\"*\")#only td elements\n",
    "            dic = {}\n",
    "            \n",
    "            #column 1\n",
    "            #name\n",
    "            names = cols[1].find_elements_by_css_selector(\"div > div > div:nth-child(2) > div\")\n",
    "            dic[name_key] = (\n",
    "                \"'\" + (\n",
    "                    names[0].get_attribute(\"innerText\") + \" \" + names[1].get_attribute(\"innerText\")\n",
    "                ).replace(\"'\", \"\\\\'\") + \"'\"\n",
    "            )\n",
    "            #info = cols[1].find_element_by_class_name(\"info\")\n",
    "            info = cols[1].find_element_by_css_selector(\"div > .bottom > ul\")\n",
    "            info_lis = info.find_elements_by_css_selector(\"li\")\n",
    "            #id (profile identifier for url)\n",
    "            dic[id_key] = int(info.find_element_by_tag_name(\"a\").get_attribute(\"href\").split(\"/\")[-1])\n",
    "            #region/division ids\n",
    "            dic[region_key] = filter_maps[\"region\"][info_lis[0].get_attribute(\"innerText\")]\n",
    "            dic[division_key] = div[0]\n",
    "            #age\n",
    "            dic[age_key] = int(info_lis[1].get_attribute(\"innerText\").split(\" \")[1])\n",
    "            #height/weight (not mandatory fields)\n",
    "            if 2 < len(info_lis):\n",
    "                height_weight = info_lis[2].get_attribute(\"innerText\").split(\" \")\n",
    "                #height\n",
    "                if \"in\" in height_weight:\n",
    "                    dic[height_key] = float(height_weight[height_weight.index(\"in\") - 1])\n",
    "                elif \"cm\" in height_weight:\n",
    "                    dic[height_key] = float(height_weight[height_weight.index(\"cm\") - 1]) / 2.54\n",
    "                else:\n",
    "                    dic[height_key] = -1\n",
    "                dic[height_key] = int(round(dic[height_key]))\n",
    "                #weight\n",
    "                if \"lb\" in height_weight:\n",
    "                    dic[weight_key] = float(height_weight[height_weight.index(\"lb\") - 1])\n",
    "                elif \"kg\" in height_weight:\n",
    "                    dic[weight_key] = float(height_weight[height_weight.index(\"kg\") - 1]) * 2.20462\n",
    "                else:\n",
    "                    dic[weight_key] = -1\n",
    "                dic[weight_key] = int(round(dic[weight_key]))\n",
    "            \n",
    "            #columns 3-8 (inclusive, 0-indexed)\n",
    "            #leaderboard workout reps, time, or weight\n",
    "            scaled = False\n",
    "            first_col = 3\n",
    "            for i in range(first_col, 9):\n",
    "                #get inner value minus the surrounding parentheses\n",
    "                html_raw = cols[i].find_element_by_css_selector(\n",
    "                    \"div > div > span > span:nth-child(2)\"\n",
    "                ).get_attribute(\"innerText\")\n",
    "                html = html_raw[html_raw.index(\"(\") + 1:-1]\n",
    "                #print(html_raw, html)\n",
    "                #if athlete scaled a workout, don't any data\n",
    "                if html.endswith(\"- s\"):\n",
    "                    scaled = True\n",
    "                    break\n",
    "                #convert values\n",
    "                key = workout_keys[i - first_col]\n",
    "                #handle html\n",
    "                dic[key] = handle_crossfit_score(html)\n",
    "\n",
    "            #append athlete if not scaled\n",
    "            if not scaled:\n",
    "                athletes.append(dic)\n",
    "        #iterate over athletes\n",
    "        for a in athletes:\n",
    "            driver.get(athlete_url.format(a[\"id\"]))\n",
    "            #wait for bottom stats to laod\n",
    "            #get leaderboard (might have to wait)\n",
    "            try:\n",
    "                #get stats\n",
    "                stats = WebDriverWait(driver, 1).until(\n",
    "                    EC.presence_of_element_located(\n",
    "                        (\n",
    "                            By.CSS_SELECTOR,\n",
    "                            \"body #athleteProfile > div:nth-last-child(3) > .container > .stats-container\"\n",
    "                        )\n",
    "                    )\n",
    "                )\n",
    "                #iterate and collect numbers\n",
    "                for i in range(1, 3):\n",
    "                    sel0 = custom_stat_selects[0].format(i)\n",
    "                    for j in range(1, 3):\n",
    "                        sel1 = custom_stat_selects[1].format(j)\n",
    "                        for k in range(1, 4):\n",
    "                            sel2 = custom_stat_selects[2].format(k)\n",
    "                            #select\n",
    "                            html = stats.find_element_by_css_selector(sel0 + sel1 + sel2).get_attribute(\"innerText\")\n",
    "                            a[stats_keys[6 * (i - 1) + 3 * (j - 1) + (k - 1)]] = handle_crossfit_score(html)\n",
    "            except:\n",
    "                #pass\n",
    "                for k in stats_keys:\n",
    "                    a[k] = -1\n",
    "            \n",
    "            #get affiliate id\n",
    "            try:\n",
    "                a[affiliate_key] = int(\n",
    "                    (\n",
    "                        driver.find_element_by_css_selector(\n",
    "                            \"body #athleteProfile > .page-cover .bg-games-black-overlay .infobar > li:nth-child(6) a\"\n",
    "                        ).get_attribute(\"href\")\n",
    "                    ).split(\"/\")[-1]\n",
    "                )\n",
    "            except:\n",
    "                a[affiliate_key] = -1\n",
    "            \n",
    "        #\"stats-container\" class may not be present on profile pages (bs, dl, cj, ...)\n",
    "        #for a in athletes:\n",
    "        #    print(\"\\n\".join([\"{}: {}\".format(k, a[k]) for k in sorted(list(a.keys()))]) + \"\\n\")\n",
    "        \n",
    "        #update database\n",
    "        try:\n",
    "            con = get_connect()\n",
    "            with con.cursor() as cur:\n",
    "                #insert athletes (if any from this page did not scale)\n",
    "                if len(athletes) != 0:\n",
    "                    sql = \"\"\"\n",
    "                    INSERT INTO athlete ({}) VALUES {};\n",
    "                    \"\"\".format(\n",
    "                        all_keys_str,\n",
    "                        \",\\n\".join([\"(\" + \",\".join([str(a[k]) for k in all_keys]) + \")\" for a in athletes])\n",
    "                    )\n",
    "                    cur.execute(sql)\n",
    "                    con.commit()\n",
    "                    \n",
    "                #increase value of current page\n",
    "                sql = \"\"\"\n",
    "                UPDATE worldwide_division_pages SET curr_page = {} WHERE division_id = {};\n",
    "                \"\"\".format(\n",
    "                    div[1] + 1,\n",
    "                    div[0]\n",
    "                )\n",
    "                cur.execute(sql)\n",
    "                con.commit()\n",
    "        finally:\n",
    "            if con:\n",
    "                con.close()\n",
    "        \n",
    "        #increase local value of current page\n",
    "        div[1] += 1\n",
    "\n",
    "#close browser\n",
    "driver.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
