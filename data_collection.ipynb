{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# data collection\n",
    "This notebook is responsible for collecting CrossFit 2018 Open Leaderboard data and athelete profile data *as represented at the time of data collection*."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## imports\n",
    "The below is just a set of import statements required to run the code in this notebook. A description for the purpose of each import statement should be commented above it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#sql connector\n",
    "import pymysql as pms\n",
    "#time recording and sleeping\n",
    "from time import time, sleep\n",
    "#browser automation\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## database credentials\n",
    "In order to connect to a local MySQL database, the block below runs to read the username, password, database name, and host required to establish a connection."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "db_user = \"\"\n",
    "db_pass = \"\"\n",
    "db_name = \"\"\n",
    "db_host = \"\"\n",
    "with open(\"database_credentials.txt\") as f:\n",
    "    db_user = f.readline().strip()\n",
    "    db_pass = f.readline().strip()\n",
    "    db_name = f.readline().strip()\n",
    "    db_host = f.readline().strip()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## test database connection\n",
    "This short snippet is going to attempt to connect to the database and drop out without doing anything. This is just to make sure the credentials and PyMySQL are working properly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_connect():\n",
    "    \"\"\"\n",
    "    Returns a database connection object using the default params\n",
    "    specified in the database_credentials file.\n",
    "    \"\"\"\n",
    "    return pms.connect(host=db_host, user=db_user, passwd=db_pass, db=db_name, charset=\"utf8\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Connected.\n",
      "Got database cursor. Can make queries within here.\n",
      "Connection closed.\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    con = get_connect()\n",
    "    print(\"Connected.\")\n",
    "    with con.cursor() as cur:\n",
    "        print(\"Got database cursor. Can make queries within here.\")\n",
    "finally:\n",
    "    if con:\n",
    "        con.close()\n",
    "        print(\"Connection closed.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## urls\n",
    "These urls/variables can be used to jump to pages where leaderboard data is available."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "default_url = \"https://games.crossfit.com/leaderboard/open/2018?division=1&region=0&scaled=0&sort=0&occupation=0&page=1\"\n",
    "custom_url = \"https://games.crossfit.com/leaderboard/open/2018?division={}&region={}&scaled={}&sort={}&occupation={}&page={}\"\n",
    "athlete_url = \"https://games.crossfit.com/athlete/{}\"\n",
    "\n",
    "#this map would be used to substitute values into the\n",
    "#custom url string at position \"division={}\" in place\n",
    "#of the \"{}\"\n",
    "map_division = {\n",
    "    \"men\": 1,\n",
    "    \"women\": 2,\n",
    "    \"team\": 11,\n",
    "    #men aged (35-39) inclusive\n",
    "    \"m35-39\": 18,\n",
    "    #women aged (35-39) inclusive\n",
    "    \"w35-39\": 19,\n",
    "    \"m40-44\": 12,\n",
    "    \"w40-44\": 13,\n",
    "    \"m45-49\": 3,\n",
    "    \"w45-49\": 4,\n",
    "    \"m50-54\": 5,\n",
    "    \"w50-54\": 6,\n",
    "    \"m55-59\": 7,\n",
    "    \"w55-59\": 8,\n",
    "    \"m60+\": 9,\n",
    "    \"w60+\": 10,\n",
    "    #boys aged (16-17) inclusive\n",
    "    \"b16-17\": 16,\n",
    "    #girls aged (16-17) inclusive\n",
    "    \"g16-17\": 17,\n",
    "    \"b14-15\": 14,\n",
    "    \"g14-15\": 15\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## getting custom url values\n",
    "Although the URL custom attributes can be harded, the more robust solution is to write a browser automation step, prior to the main data collection, that acquires the custom url attributes corresponding to each filter. This is done below.\n",
    "\n",
    "The goal is to obtain all of the maps like the one above in a more robust manner.\n",
    "\n",
    "Also, in the below step, it's important to note that the year and competition can also be used to filter results. However, at the current time, regional data is not available for 2018 (the open just finished). Furthermore, previous open leaderboard have different HTML structure (would require additional scraping code), and I really only care about 2018. Additionally, Rx'd/scaled, per-workout, occupation, and region are also available filtering criteria.\n",
    "\n",
    "**Rx'd/scaled and occupation**\n",
    "I don't care about these for the time being. This repo will only consider non-specific occupation and Rx'd athletes.\n",
    "\n",
    "**per-workout and region**\n",
    "I'll be able to do this filtering on my own (hypothetically). In order to do so, the region and per-workout scores will be scraped from the leaderboard and filtered on later in other notebooks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "HTML filters which will be used for filtering:\n",
      "['division', 'region']\n"
     ]
    }
   ],
   "source": [
    "filters = [\"division\", \"region\"]\n",
    "print(\"HTML filters which will be used for filtering:\\n{}\".format(filters))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below we're going to go to the leaderboard (autonomously) and scrape the IDs [CrossFit](https://games.crossfit.com/leaderboard/open/2018?division=1&region=0&scaled=0&sort=0&occupation=0&page=1) uses for divisions and regions. Although I don't need to use the same IDs, it can only help to use the same mappings. When we're talking about IDs here, I mean the numeric values that would be used to plug into the `\"{}\"` occurences in the `custom_url` string above.\n",
    "\n",
    "Here's the documentation I use for [Selenium](http://selenium-python.readthedocs.io/locating-elements.html)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\unknown\\AppData\\Local\\conda\\conda\\envs\\crossfit_open_2018\\lib\\site-packages\\pymysql\\cursors.py:103: Warning: (1050, \"Table 'division' already exists\")\n",
      "  return self._nextset(False)\n"
     ]
    }
   ],
   "source": [
    "#attempt database connect\n",
    "try:\n",
    "    con = get_connect()\n",
    "    with con.cursor() as cur:\n",
    "        #create division and region tables if they don't exist\n",
    "        sql = \"\"\"\n",
    "        CREATE TABLE IF NOT EXISTS region (\n",
    "            id INT PRIMARY KEY,\n",
    "            region VARCHAR(24) NOT NULL\n",
    "        );\n",
    "        CREATE TABLE IF NOT EXISTS division (\n",
    "            id INT PRIMARY KEY,\n",
    "            division VARCHAR(16) NOT NULL\n",
    "        );\n",
    "        \"\"\"\n",
    "        cur.execute(sql)\n",
    "        \n",
    "        #attempt to get regions and divisions\n",
    "        result_counts = [-1, -1]\n",
    "        sql = \"\"\"\n",
    "        SELECT * FROM {};\n",
    "        \"\"\"\n",
    "        for i in range(len(filters)):\n",
    "            cur.execute(sql.format(filters[i]))\n",
    "            result = cur.fetchall()\n",
    "            #store number of results\n",
    "            result_counts[i] = len(result)\n",
    "            #output results\n",
    "            #print(\"==== {} results({}) ====\".format(filters[i], result_counts[i]))\n",
    "            #print(\", \".join([col[0] for col in cur.description]))\n",
    "            for j in range(result_counts[i]):\n",
    "                break\n",
    "                #print(\"{}: {}\".format(j, result[j]))\n",
    "        \n",
    "        #if results for both are not empty, the values have\n",
    "        #already been scraped, so skip this\n",
    "        if result_counts[0] < 1 or result_counts[1] < 1:\n",
    "            #Store id, region/div pairs as tuples (id_0, region_0/div_0)\n",
    "            #the below entries will have 2 lists of such tuples, 1 for\n",
    "            #each filter_id\n",
    "            entries = []\n",
    "            #spin up browser\n",
    "            driver = webdriver.Chrome()\n",
    "            driver.get(default_url)\n",
    "            \n",
    "            #iterate over useful filters\n",
    "            for i in range(len(filters)):\n",
    "                #ids are formatted with control- as a prefix\n",
    "                dropdown = driver.find_element_by_id(\"control-\" + filters[i])\n",
    "                options = dropdown.find_elements_by_tag_name(\"option\")\n",
    "                #aggregate entries\n",
    "                #also, this is making 2 calls to o.get_attribute(\"value\"), and this\n",
    "                #could be done \"more efficiently\" without the list comprehension\n",
    "                entries.append([(int(o.get_attribute(\"value\")), o.get_attribute(\"innerText\"))\n",
    "                            for o in options if o.get_attribute(\"value\") != \"\"])\n",
    "                #print(entries[i])\n",
    "            \n",
    "            #close driver\n",
    "            driver.close()\n",
    "        \n",
    "            #write entries to file\n",
    "            sql = \"\"\"\n",
    "            INSERT INTO {}(id, {}) VALUES\n",
    "                {}\n",
    "            \"\"\"\n",
    "            for i in range(len(filters)):\n",
    "                cur.execute(sql.format(filters[i], filters[i],\n",
    "                                       \",\\n\".join([\"({}, '{}')\".format(e[0], e[1]) for e in entries[i]])))\n",
    "            #commit inserts\n",
    "            con.commit()\n",
    "finally:\n",
    "    if con:\n",
    "        con.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## checkout filter values\n",
    "Now that we've ensured the filters are in the database, let's grab them and create mappings. These will be necessary for the athlete table."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#this map will contain maps for each filter\n",
    "filter_maps = {}\n",
    "try:\n",
    "    con = get_connect()\n",
    "    with con.cursor() as cur:\n",
    "        sql = \"\"\"\n",
    "        SELECT * FROM {};\n",
    "        \"\"\"\n",
    "        for f in filters:\n",
    "            #add filter map\n",
    "            filter_maps[f] = {}\n",
    "            #get results\n",
    "            cur.execute(sql.format(f))\n",
    "            result = cur.fetchall()\n",
    "            #store results\n",
    "            for r in result:\n",
    "                #create mapping from div/region -> id or vice-versa (depending on scraping needs)\n",
    "                if f == \"division\":\n",
    "                    filter_maps[f][r[0]] = r[1]\n",
    "                elif f == \"region\":\n",
    "                    filter_maps[f][r[1]] = r[0]\n",
    "finally:\n",
    "    if con:\n",
    "        con.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#filter_maps"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## hard-coded values\n",
    "At the time of data collection, the 2018 CrossFit Open has ended. With this, we're making the assumption the total number of pages for each leaderboard will not change (this is not necessarily 100%, but I'm assuming it's very close to 100%). The leaderboard is structured in a way such that if a page is requested beyond the total number of leaderboard pages available for a specific filter, it redirects back to the first leaderboard page.\n",
    "\n",
    "Although there are different ways to handle this, in order to know when to stop scraping for a specific filter, we'll just scrape the index of the last available page, which is available at the bottom of each leaderboard. Additionally, the current index for each filter will also scraped, defaulting to the first leaderboard page: 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\unknown\\AppData\\Local\\conda\\conda\\envs\\crossfit_open_2018\\lib\\site-packages\\pymysql\\cursors.py:166: Warning: (1050, \"Table 'worldwide_division_pages' already exists\")\n",
      "  result = self._query(query)\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    con = get_connect()\n",
    "    with con.cursor() as cur:\n",
    "        #create database to store these worldwide-division indices\n",
    "        #(could easily be stored in a flat file instead)\n",
    "        sql = \"\"\"\n",
    "        CREATE TABLE IF NOT EXISTS worldwide_division_pages (\n",
    "            division_id INT PRIMARY KEY,\n",
    "            FOREIGN KEY (division_id)\n",
    "                REFERENCES division(id)\n",
    "                ON DELETE CASCADE,\n",
    "            curr_page INT NOT NULL DEFAULT 1,\n",
    "            last_page INT NOT NULL DEFAULT -1\n",
    "        )\n",
    "        \"\"\"\n",
    "        cur.execute(sql)\n",
    "        \n",
    "        #check if table has been populated already in a previous run\n",
    "        sql = \"\"\"\n",
    "        SELECT * FROM worldwide_division_pages;\n",
    "        \"\"\"\n",
    "        cur.execute(sql)\n",
    "        result = cur.fetchall()\n",
    "        if len(result) == 0:\n",
    "            #cross-populate division id's from division table\n",
    "            sql = \"\"\"\n",
    "            INSERT INTO worldwide_division_pages (division_id)\n",
    "                SELECT id FROM division;\n",
    "            \"\"\"\n",
    "            cur.execute(sql)\n",
    "            con.commit()\n",
    "            \n",
    "            #grab option IDs from database\n",
    "            #this could be done by just collecting them from the browser, the values\n",
    "            #are supposed to be identical\n",
    "            sql = \"\"\"\n",
    "            SELECT division_id FROM worldwide_division_pages;\n",
    "            \"\"\"\n",
    "            cur.execute(sql)\n",
    "            result = cur.fetchall()\n",
    "            ids = [r[0] for r in result]\n",
    "            print(ids)\n",
    "                     \n",
    "            #scrape last_page values for each division\n",
    "            driver = webdriver.Chrome()\n",
    "            driver.get(default_url)\n",
    "            \n",
    "            #grab division selectable and store in browser\n",
    "            inject_store_select = \"\"\"\n",
    "            window.division_select = document.getElementById(\"control-division\");\n",
    "            \"\"\"\n",
    "            driver.execute_script(inject_store_select)\n",
    "            \n",
    "            last_pages = []\n",
    "            #iterate over ids\n",
    "            for i in ids:\n",
    "                #force select element to change to new dropdown option\n",
    "                inject_change_select = \"\"\"\n",
    "                window.division_select.value = {};\n",
    "                window.division_select.dispatchEvent(new Event(\"change\"));\n",
    "                \"\"\".format(i)\n",
    "                driver.execute_script(inject_change_select)\n",
    "                #wait for page to update\n",
    "                sleep(2)\n",
    "                #grab last page text from the bottom\n",
    "                last_pages.append(int(\n",
    "                    driver.find_element_by_class_name(\"nums\")\n",
    "                        .find_elements_by_tag_name(\"a\")[-1]\n",
    "                        .get_attribute(\"innerText\")\n",
    "                ))\n",
    "            \n",
    "            #write updates to file in bulk\n",
    "            sql = \"\"\"\n",
    "            INSERT INTO worldwide_division_pages(division_id, last_page) VALUES\n",
    "                {}\n",
    "                ON DUPLICATE KEY UPDATE last_page = VALUES(last_page);\n",
    "            \"\"\"\n",
    "            cur.execute(\n",
    "                sql.format(\n",
    "                    \",\\n\".join([\"({}, '{}')\".format(ids[i], last_pages[i]) for i in range(len(ids))])\n",
    "                )\n",
    "            )\n",
    "            con.commit()\n",
    "            \n",
    "            #close driver\n",
    "            driver.close()\n",
    "finally:\n",
    "    if con:\n",
    "        con.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### additional fixed filters\n",
    "Below are the hard-coded values for scaled, occupation, sort, and workout type."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "custom url:\n",
      "https://games.crossfit.com/leaderboard/open/2018?division={}&region=0&scaled=0&sort=0&occupation=0&page={}\n"
     ]
    }
   ],
   "source": [
    "region = 0\n",
    "scaled = 0\n",
    "sort = 0\n",
    "occupation = 0\n",
    "custom_url = (\n",
    "    \"https://games.crossfit.com/leaderboard/open/2018?division={}&region={}&scaled={}&sort={}&occupation={}&page={}\"\n",
    "        .format(\"{}\", region, scaled, sort, occupation, \"{}\")\n",
    ")\n",
    "print(\"custom url:\\n{}\".format(custom_url))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## leaderboard/athlete scraping overview\n",
    "Scraping the Open leaderboard data is only half of the data required per athlete. The remaining data will be collected from their athlete profile, containing statistics for their Back Squat, Fran, and other CrossFit staples. This will be done **after** the Open data for a specific division is completely finished. Therefore, the scraping process pipeline from this point forward is as follows:\n",
    "* for each division:\n",
    "    * for each leaderboard page:\n",
    "        * scrape all athlete leaderboard information\n",
    "        * for each athlete:\n",
    "            * scrape profile data\n",
    "        * write all athlete data to file\n",
    "        * update current page for this division"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### reading in the pages to be scraped from the Open leaderboard\n",
    "Below the pages left to be scraped will be collected from the database and sorted in order from least athletes to most athletes **per division**. Divisions which have complete Open leaderboard data already scraped will have a current page value exceeding the last page value by 1 (`curr_page = 3`, `last_page = 2`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "try:\n",
    "    con = get_connect()\n",
    "    with con.cursor() as cur:\n",
    "        sql = \"\"\"\n",
    "        SELECT * FROM worldwide_division_pages;\n",
    "        \"\"\"\n",
    "        cur.execute(sql)\n",
    "        #sort results based on last_page value\n",
    "        division_pages = sorted(\n",
    "            list(\n",
    "                map(\n",
    "                    lambda tup: list(tup),\n",
    "                    cur.fetchall()\n",
    "                )\n",
    "            ), \n",
    "            key=lambda tup: tup[2],\n",
    "            reverse=True\n",
    "        )\n",
    "finally:\n",
    "    if con:\n",
    "        con.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#division_pages"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## setting up the athlete tables\n",
    "The athlete table will contain data for athletes from 2 sources: leaderboards and profiles. The data from each in it's native form is shown below.\n",
    "\n",
    "### leaderboards\n",
    "<img src=\"images/leaderboard.png\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### profiles\n",
    "<img src=\"images/basic_stats.png\" />\n",
    "<img src=\"images/benchmark_stats.png\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Although this data could be separated into 2 or 3 different tables, for the scope of the planned analysis, it can all go into the same table. The collected data per athlete will contain the following:\n",
    "* leaderboard\n",
    "    * name\n",
    "    * 18.1, 18.2, 18.2a, 18.3, 18.4, 18.5 scores (not rank, a.k.a. time or reps or weight)\n",
    "    * height, weight, age\n",
    "    * region, division\n",
    "* profile\n",
    "    * affiliate\n",
    "    * back squat, clean and jerk, snatch, deadlift\n",
    "    * fight gone bad, fran, grace, helen, filthy 50\n",
    "    * max pull-ups\n",
    "    * sprint 400m, run 5k\n",
    "    \n",
    "The table containing all this data is created below. Time data will be recorded in seconds, weight in pounds, height in inches, and age in years."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\unknown\\AppData\\Local\\conda\\conda\\envs\\crossfit_open_2018\\lib\\site-packages\\pymysql\\cursors.py:166: Warning: (1050, \"Table 'athlete' already exists\")\n",
      "  result = self._query(query)\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    con = get_connect()\n",
    "    with con.cursor() as cur:\n",
    "        sql = \"\"\"\n",
    "        CREATE TABLE IF NOT EXISTS athlete (\n",
    "            id INT PRIMARY KEY,\n",
    "            name VARCHAR(128) NOT NULL,\n",
    "            leaderboard_18_1_reps INT,\n",
    "            leaderboard_18_2_time_secs INT,\n",
    "            leaderboard_18_2a_weight_lbs INT,\n",
    "            leaderboard_18_3_reps INT,\n",
    "            leaderboard_18_4_time_secs INT,\n",
    "            leaderboard_18_5_reps INT,\n",
    "            \n",
    "            height_in INT,\n",
    "            weight_lbs INT,\n",
    "            age_years INT,\n",
    "            \n",
    "            region_id INT NOT NULL,\n",
    "            FOREIGN KEY (region_id)\n",
    "                REFERENCES region(id)\n",
    "                ON DELETE CASCADE,\n",
    "            division_id INT NOT NULL,\n",
    "            FOREIGN KEY (division_id)\n",
    "                REFERENCES division(id)\n",
    "                ON DELETE CASCADE,\n",
    "            \n",
    "            affiliate_id INT,\n",
    "            \n",
    "            back_squat_lbs INT,\n",
    "            clean_and_jerk_lbs INT,\n",
    "            snatch_lbs INT,\n",
    "            deadlift_lbs INT,\n",
    "            \n",
    "            fight_gone_bad_time_secs INT,\n",
    "            fran_time_secs INT,\n",
    "            grace_time_secs INT,\n",
    "            helen_time_secs INT,\n",
    "            filthy_50_time_secs INT,\n",
    "            \n",
    "            max_pull_ups INT,\n",
    "            \n",
    "            sprint_400_m_time_secs INT,\n",
    "            run_5_km_time_secs INT\n",
    "        );\n",
    "        \"\"\"\n",
    "        cur.execute(sql)\n",
    "finally:\n",
    "    if con:\n",
    "        con.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## scrape athlete data\n",
    "Here we go. glhf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#used for output styling\n",
    "center_space = 60\n",
    "center_sep = \" \"\n",
    "\n",
    "#for leaderboard workouts\n",
    "workout_keys = [\n",
    "    \"18_1_reps\", \"18_2_time_secs\", \"18_2a_weight_lbs\", \"18_3_reps\", \"18_4_time_secs\", \"18_5_reps\"]\n",
    "workout_keys = list(map(lambda k: \"leaderboard_\" + k, workout_keys))\n",
    "\n",
    "stats_keys = [\n",
    "    \"back_squat_lbs\",\n",
    "    \"clean_and_jerk_lbs\",\n",
    "    \"snatch_lbs\",\n",
    "    \n",
    "    \"deadlift_lbs\",\n",
    "    \"fight_gone_bad_time_secs\",\n",
    "    \"max_pull_ups\",\n",
    "    \n",
    "    \"fran_time_secs\",\n",
    "    \"grace_time_secs\",\n",
    "    \"helen_time_secs\",\n",
    "    \n",
    "    \"filthy_50_time_secs\",\n",
    "    \"sprint_400_m_time_secs\",\n",
    "    \"run_5_km_time_secs\"\n",
    "]\n",
    "\n",
    "#for other values (height, weight, etc.)\n",
    "height_key = \"height_in\"\n",
    "weight_key = \"weight_lbs\"\n",
    "age_key = \"age_years\"\n",
    "id_key = \"id\"\n",
    "region_key = \"region_id\"\n",
    "division_key = \"division_id\"\n",
    "name_key = \"name\"\n",
    "affiliate_key = \"affiliate_id\"\n",
    "\n",
    "#all keys (used for insertion)\n",
    "all_keys = sorted(workout_keys + stats_keys + [\n",
    "    height_key, weight_key, age_key, id_key, region_key, division_key, name_key, affiliate_key\n",
    "])\n",
    "all_keys_str = \", \".join(all_keys)\n",
    "\n",
    "#time map values for converting to seconds\n",
    "time_mults = [3600, 60, 1]\n",
    "\n",
    "#used for getting statistics on athlete profile page\n",
    "custom_stat_selects = [\n",
    "    \"li:nth-child({})\",\n",
    "    \"> .stats-section:nth-child({}) \",\n",
    "    \"> table > tbody > tr:nth-child({}) > td\"\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def handle_crossfit_score(html):\n",
    "    \"\"\"\n",
    "    Handles the parsing of several different CrossFit scores such as\n",
    "    weight (lb), time (x:y:z), reps (reps)/(), or none (--). Returns the\n",
    "    vale parsed to it's database required equivalent.\n",
    "    \"\"\"\n",
    "    #reps\n",
    "    if html[-4:] == \"reps\":\n",
    "        return int(html[:-5])\n",
    "    #time\n",
    "    elif \":\" in html:\n",
    "        hrs_mins_secs = list(map(lambda x: int(x), html.split(\":\")))\n",
    "        len_diff = len(time_mults) - len(hrs_mins_secs)\n",
    "        #collect all seconds\n",
    "        return sum(\n",
    "            [hrs_mins_secs[i] * time_mults[i + len_diff]\n",
    "                 for i in range(len(hrs_mins_secs))]\n",
    "        )\n",
    "    #no score\n",
    "    elif html == \"--\":\n",
    "        return -1\n",
    "    #weight\n",
    "    elif html[-2:] == \"lb\" or html[-2:] == \"kg\":\n",
    "        weight = int(html[:-3])\n",
    "        return weight if html[-2:] == \"lb\" else int(round(weight * 2.2))\n",
    "    #reps with no reps on the end (used for max pull-ups in athlete profile)\n",
    "    else:\n",
    "        #handle special values with -2 entry (200 reps for pullups is not happening in 2018)\n",
    "        parsed_val = int(html)\n",
    "        return parsed_val if parsed_val < 150 else -2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                Division Men\n",
      "Page 215\n",
      "Page 216\n",
      "Page 217\n",
      "Page 218\n",
      "Page 219\n",
      "Page 220\n",
      "Page 221\n",
      "Page 222\n",
      "Page 223\n",
      "Page 224\n",
      "Page 225\n",
      "Page 226\n",
      "Page 227\n",
      "Page 228\n",
      "Page 229\n",
      "Page 230\n",
      "Page 231\n",
      "Page 232\n",
      "Page 233\n",
      "Page 234\n",
      "Page 235\n",
      "Page 236\n",
      "Page 237\n",
      "Page 238\n",
      "Page 239\n",
      "Page 240\n",
      "Page 241\n",
      "Page 242\n",
      "Page 243\n",
      "Page 244\n",
      "Page 245\n",
      "Page 246\n",
      "Page 247\n",
      "Page 248\n",
      "Page 249\n",
      "Page 250\n",
      "Page 251\n",
      "Page 252\n",
      "Page 253\n",
      "Page 254\n",
      "Page 255\n",
      "Page 256\n",
      "Page 257\n",
      "Page 258\n",
      "Page 259\n",
      "Page 260\n",
      "Page 261\n",
      "Page 262\n",
      "Page 263\n",
      "Page 264\n",
      "Page 265\n",
      "Page 266\n",
      "Page 267\n",
      "Page 268\n",
      "Page 269\n",
      "Page 270\n",
      "Page 271\n",
      "Page 272\n",
      "Page 273\n",
      "Page 274\n",
      "Page 275\n",
      "Page 276\n",
      "Page 277\n",
      "Page 278\n",
      "Page 279\n",
      "Page 280\n",
      "Page 281\n",
      "Page 282\n",
      "Page 283\n",
      "Page 284\n",
      "Page 285\n",
      "Page 286\n",
      "Page 287\n",
      "Page 288\n",
      "Page 289\n",
      "Page 290\n",
      "Page 291\n",
      "Page 292\n",
      "Page 293\n",
      "Page 294\n",
      "Page 295\n",
      "Page 296\n",
      "Page 297\n",
      "Page 298\n",
      "Page 299\n",
      "Page 300\n",
      "Page 301\n",
      "Page 302\n",
      "Page 303\n",
      "Page 304\n",
      "Page 305\n",
      "Page 306\n",
      "Page 307\n",
      "Page 308\n",
      "Page 309\n",
      "Page 310\n",
      "Page 311\n",
      "Page 312\n",
      "Page 313\n",
      "Page 314\n",
      "Page 315\n",
      "Page 316\n",
      "Page 317\n",
      "Page 318\n",
      "Page 319\n",
      "Page 320\n",
      "Page 321\n",
      "Page 322\n",
      "Page 323\n",
      "Page 324\n",
      "Page 325\n",
      "Page 326\n",
      "Page 327\n",
      "Page 328\n",
      "Page 329\n",
      "Page 330\n",
      "Page 331\n",
      "Page 332\n",
      "Page 333\n",
      "Page 334\n",
      "Page 335\n",
      "Page 336\n",
      "Page 337\n",
      "Page 338\n",
      "Page 339\n",
      "Page 340\n",
      "Page 341\n",
      "Page 342\n",
      "Page 343\n",
      "Page 344\n",
      "Page 345\n",
      "Page 346\n",
      "Page 347\n",
      "Page 348\n",
      "Page 349\n",
      "Page 350\n",
      "Page 351\n",
      "Page 352\n",
      "Page 353\n",
      "Page 354\n",
      "Page 355\n",
      "Page 356\n",
      "Page 357\n",
      "Page 358\n",
      "Page 359\n",
      "Page 360\n",
      "Page 361\n",
      "Page 362\n",
      "Page 363\n",
      "Page 364\n",
      "Page 365\n",
      "Page 366\n",
      "Page 367\n",
      "Page 368\n",
      "Page 369\n",
      "Page 370\n",
      "Page 371\n",
      "Page 372\n",
      "Page 373\n",
      "Page 374\n",
      "Page 375\n",
      "Page 376\n",
      "Page 377\n",
      "Page 378\n",
      "Page 379\n",
      "Page 380\n",
      "Page 381\n",
      "Page 382\n",
      "Page 383\n",
      "Page 384\n",
      "Page 385\n",
      "Page 386\n",
      "Page 387\n",
      "Page 388\n",
      "Page 389\n",
      "Page 390\n",
      "Page 391\n",
      "Page 392\n",
      "Page 393\n",
      "Page 394\n",
      "Page 395\n",
      "Page 396\n",
      "Page 397\n",
      "Page 398\n",
      "Page 399\n",
      "Page 400\n",
      "Page 401\n",
      "Page 402\n",
      "Page 403\n",
      "Page 404\n",
      "Page 405\n",
      "Page 406\n",
      "Page 407\n",
      "Page 408\n",
      "Page 409\n",
      "Page 410\n",
      "Page 411\n",
      "Page 412\n",
      "Page 413\n",
      "Page 414\n",
      "Page 415\n",
      "Page 416\n",
      "Page 417\n",
      "Page 418\n",
      "Page 419\n",
      "Page 420\n",
      "Page 421\n",
      "Page 422\n",
      "Page 423\n",
      "Page 424\n",
      "Page 425\n",
      "Page 426\n",
      "Page 427\n",
      "Page 428\n",
      "Page 429\n",
      "Page 430\n",
      "Page 431\n",
      "Page 432\n",
      "Page 433\n",
      "Page 434\n",
      "Page 435\n",
      "Page 436\n",
      "Page 437\n",
      "Page 438\n",
      "Page 439\n",
      "Page 440\n",
      "Page 441\n",
      "Page 442\n",
      "Page 443\n",
      "Page 444\n",
      "Page 445\n",
      "Page 446\n",
      "Page 447\n",
      "Page 448\n",
      "Page 449\n",
      "Page 450\n",
      "Page 451\n",
      "Page 452\n",
      "Page 453\n",
      "Page 454\n",
      "Page 455\n",
      "Page 456\n",
      "Page 457\n",
      "Page 458\n",
      "Page 459\n",
      "Page 460\n",
      "Page 461\n",
      "Page 462\n",
      "Page 463\n",
      "Page 464\n",
      "Page 465\n",
      "Page 466\n",
      "Page 467\n",
      "Page 468\n",
      "Page 469\n",
      "Page 470\n",
      "Page 471\n",
      "Page 472\n",
      "Page 473\n",
      "Page 474\n",
      "Page 475\n",
      "Page 476\n",
      "Page 477\n",
      "Page 478\n",
      "Page 479\n",
      "Page 480\n",
      "Page 481\n",
      "Page 482\n",
      "Page 483\n",
      "Page 484\n",
      "Page 485\n",
      "Page 486\n",
      "Page 487\n",
      "Page 488\n",
      "Page 489\n",
      "Page 490\n",
      "Page 491\n",
      "Page 492\n",
      "Page 493\n",
      "Page 494\n",
      "Page 495\n",
      "Page 496\n",
      "Page 497\n",
      "Page 498\n",
      "Page 499\n",
      "Page 500\n",
      "Page 501\n",
      "Page 502\n",
      "Page 503\n",
      "Page 504\n",
      "Page 505\n",
      "Page 506\n",
      "Page 507\n",
      "Page 508\n",
      "Page 509\n",
      "Page 510\n",
      "Page 511\n",
      "Page 512\n",
      "Page 513\n",
      "Page 514\n",
      "Page 515\n",
      "Page 516\n",
      "Page 517\n",
      "Page 518\n",
      "Page 519\n",
      "Page 520\n",
      "Page 521\n",
      "Page 522\n",
      "Page 523\n",
      "Page 524\n",
      "Page 525\n",
      "Page 526\n",
      "Page 527\n",
      "Page 528\n",
      "Page 529\n",
      "Page 530\n",
      "Page 531\n",
      "Page 532\n",
      "Page 533\n",
      "Page 534\n",
      "Page 535\n",
      "Page 536\n",
      "Page 537\n",
      "Page 538\n",
      "Page 539\n",
      "Page 540\n",
      "Page 541\n",
      "Page 542\n",
      "Page 543\n",
      "Page 544\n",
      "Page 545\n",
      "Page 546\n",
      "Page 547\n",
      "Page 548\n",
      "Page 549\n",
      "Page 550\n",
      "Page 551\n",
      "Page 552\n",
      "Page 553\n",
      "Page 554\n",
      "Page 555\n",
      "Page 556\n",
      "Page 557\n",
      "Page 558\n",
      "Page 559\n",
      "Page 560\n",
      "Page 561\n",
      "Page 562\n",
      "Page 563\n",
      "Page 564\n",
      "Page 565\n",
      "Page 566\n",
      "Page 567\n",
      "Page 568\n",
      "Page 569\n",
      "Page 570\n",
      "Page 571\n",
      "Page 572\n",
      "Page 573\n",
      "Page 574\n",
      "Page 575\n",
      "Page 576\n",
      "Page 577\n",
      "Page 578\n",
      "Page 579\n",
      "Page 580\n",
      "Page 581\n",
      "Page 582\n",
      "Page 583\n",
      "Page 584\n",
      "Page 585\n",
      "Page 586\n",
      "Page 587\n",
      "Page 588\n",
      "Page 589\n",
      "Page 590\n",
      "Page 591\n",
      "Page 592\n",
      "Page 593\n",
      "Page 594\n",
      "Page 595\n",
      "Page 596\n",
      "Page 597\n",
      "Page 598\n",
      "Page 599\n",
      "Page 600\n",
      "Page 601\n",
      "Page 602\n",
      "Page 603\n",
      "Page 604\n",
      "Page 605\n",
      "Page 606\n",
      "Page 607\n",
      "Page 608\n",
      "Page 609\n",
      "Page 610\n",
      "Page 611\n",
      "Page 612\n",
      "Page 613\n",
      "Page 614\n",
      "Page 615\n",
      "Page 616\n",
      "Page 617\n",
      "Page 618\n",
      "Page 619\n",
      "Page 620\n",
      "Page 621\n",
      "Page 622\n",
      "Page 623\n",
      "Page 624\n",
      "Page 625\n",
      "Page 626\n",
      "Page 627\n",
      "Page 628\n",
      "Page 629\n",
      "Page 630\n",
      "Page 631\n",
      "Page 632\n",
      "Page 633\n",
      "Page 634\n",
      "Page 635\n",
      "Page 636\n",
      "Page 637\n",
      "Page 638\n",
      "Page 639\n",
      "Page 640\n",
      "Page 641\n",
      "Page 642\n"
     ]
    }
   ],
   "source": [
    "#spinup driver\n",
    "driver = webdriver.Chrome()\n",
    "\n",
    "#for each division\n",
    "for div in division_pages:\n",
    "    #output division\n",
    "    print(\"Division {}\".format(filter_maps[\"division\"][div[0]])\n",
    "             .rjust(center_space, center_sep))\n",
    "    #continue until current page exceeds last page\n",
    "    while div[1] <= div[2]:\n",
    "        print(\"Page {}\".format(div[1]))\n",
    "        \n",
    "        #go to page\n",
    "        driver.get(custom_url.format(div[0], div[1]))\n",
    "        \n",
    "        #get leaderboard (might have to wait)\n",
    "        lb = WebDriverWait(driver, 5).until(\n",
    "            EC.presence_of_element_located(\n",
    "                (\n",
    "                    By.CSS_SELECTOR,\n",
    "                    \"body > #containerOverlay > #leaderboard > .lb-main > .inner > table > tbody\"\n",
    "                )\n",
    "            )\n",
    "        )\n",
    "        #get rows\n",
    "        rows = lb.find_elements_by_xpath(\"*\")\n",
    "        #collect athlete data\n",
    "        athletes = []\n",
    "        for r in rows:\n",
    "            #get columns and create empty dictionary\n",
    "            cols = r.find_elements_by_xpath(\"*\")#only td elements\n",
    "            dic = {}\n",
    "            \n",
    "            #column 1\n",
    "            #name\n",
    "            names = cols[1].find_elements_by_css_selector(\"div > div > div:nth-child(2) > div\")\n",
    "            dic[name_key] = (\n",
    "                \"'\" + (\n",
    "                    names[0].get_attribute(\"innerText\") + \" \" + names[1].get_attribute(\"innerText\")\n",
    "                ).replace(\"'\", \"\\\\'\") + \"'\"\n",
    "            )\n",
    "            #info = cols[1].find_element_by_class_name(\"info\")\n",
    "            info = cols[1].find_element_by_css_selector(\"div > .bottom > ul\")\n",
    "            info_lis = info.find_elements_by_css_selector(\"li\")\n",
    "            #id (profile identifier for url)\n",
    "            dic[id_key] = int(info.find_element_by_tag_name(\"a\").get_attribute(\"href\").split(\"/\")[-1])\n",
    "            #region/division ids\n",
    "            dic[region_key] = filter_maps[\"region\"][info_lis[0].get_attribute(\"innerText\")]\n",
    "            dic[division_key] = div[0]\n",
    "            #age\n",
    "            dic[age_key] = int(info_lis[1].get_attribute(\"innerText\").split(\" \")[1])\n",
    "            #height/weight (not mandatory fields)\n",
    "            if 2 < len(info_lis):\n",
    "                height_weight = info_lis[2].get_attribute(\"innerText\").split(\" \")\n",
    "                #height\n",
    "                if \"in\" in height_weight:\n",
    "                    dic[height_key] = float(height_weight[height_weight.index(\"in\") - 1])\n",
    "                elif \"cm\" in height_weight:\n",
    "                    dic[height_key] = float(height_weight[height_weight.index(\"cm\") - 1]) / 2.54\n",
    "                else:\n",
    "                    dic[height_key] = -1\n",
    "                dic[height_key] = int(round(dic[height_key]))\n",
    "                #weight\n",
    "                if \"lb\" in height_weight:\n",
    "                    dic[weight_key] = float(height_weight[height_weight.index(\"lb\") - 1])\n",
    "                elif \"kg\" in height_weight:\n",
    "                    dic[weight_key] = float(height_weight[height_weight.index(\"kg\") - 1]) * 2.20462\n",
    "                else:\n",
    "                    dic[weight_key] = -1\n",
    "                dic[weight_key] = int(round(dic[weight_key]))\n",
    "            \n",
    "            #columns 3-8 (inclusive, 0-indexed)\n",
    "            #leaderboard workout reps, time, or weight\n",
    "            scaled = False\n",
    "            first_col = 3\n",
    "            for i in range(first_col, 9):\n",
    "                #get inner value minus the surrounding parentheses\n",
    "                html_raw = cols[i].find_element_by_css_selector(\n",
    "                    \"div > div > span > span:nth-child(2)\"\n",
    "                ).get_attribute(\"innerText\")\n",
    "                html = html_raw[html_raw.index(\"(\") + 1:-1]\n",
    "                #print(html_raw, html)\n",
    "                #if athlete scaled a workout, don't any data\n",
    "                if html.endswith(\"- s\"):\n",
    "                    scaled = True\n",
    "                    break\n",
    "                #convert values\n",
    "                key = workout_keys[i - first_col]\n",
    "                #handle html\n",
    "                dic[key] = handle_crossfit_score(html)\n",
    "\n",
    "            #append athlete if not scaled\n",
    "            if not scaled:\n",
    "                athletes.append(dic)\n",
    "        #iterate over athletes\n",
    "        for a in athletes:\n",
    "            driver.get(athlete_url.format(a[\"id\"]))\n",
    "            #wait for bottom stats to laod\n",
    "            #get leaderboard (might have to wait)\n",
    "            try:\n",
    "                #get stats\n",
    "                stats = WebDriverWait(driver, 1).until(\n",
    "                    EC.presence_of_element_located(\n",
    "                        (\n",
    "                            By.CSS_SELECTOR,\n",
    "                            \"body #athleteProfile > div:nth-last-child(3) > .container > .stats-container\"\n",
    "                        )\n",
    "                    )\n",
    "                )\n",
    "                #iterate and collect numbers\n",
    "                for i in range(1, 3):\n",
    "                    sel0 = custom_stat_selects[0].format(i)\n",
    "                    for j in range(1, 3):\n",
    "                        sel1 = custom_stat_selects[1].format(j)\n",
    "                        for k in range(1, 4):\n",
    "                            sel2 = custom_stat_selects[2].format(k)\n",
    "                            #select\n",
    "                            html = stats.find_element_by_css_selector(sel0 + sel1 + sel2).get_attribute(\"innerText\")\n",
    "                            a[stats_keys[6 * (i - 1) + 3 * (j - 1) + (k - 1)]] = handle_crossfit_score(html)\n",
    "            except:\n",
    "                #pass\n",
    "                for k in stats_keys:\n",
    "                    a[k] = -1\n",
    "            \n",
    "            #get affiliate id\n",
    "            try:\n",
    "                a[affiliate_key] = int(\n",
    "                    (\n",
    "                        driver.find_element_by_css_selector(\n",
    "                            \"body #athleteProfile > .page-cover .bg-games-black-overlay .infobar > li:nth-child(6) a\"\n",
    "                        ).get_attribute(\"href\")\n",
    "                    ).split(\"/\")[-1]\n",
    "                )\n",
    "            except:\n",
    "                a[affiliate_key] = -1\n",
    "            \n",
    "        #\"stats-container\" class may not be present on profile pages (bs, dl, cj, ...)\n",
    "        #for a in athletes:\n",
    "        #    print(\"\\n\".join([\"{}: {}\".format(k, a[k]) for k in sorted(list(a.keys()))]) + \"\\n\")\n",
    "        \n",
    "        #update database\n",
    "        try:\n",
    "            con = get_connect()\n",
    "            with con.cursor() as cur:\n",
    "                #insert athletes (if any from this page did not scale)\n",
    "                if len(athletes) != 0:\n",
    "                    sql = \"\"\"\n",
    "                    INSERT INTO athlete ({}) VALUES {};\n",
    "                    \"\"\".format(\n",
    "                        all_keys_str,\n",
    "                        \",\\n\".join([\"(\" + \",\".join([str(a[k]) for k in all_keys]) + \")\" for a in athletes])\n",
    "                    )\n",
    "                    cur.execute(sql)\n",
    "                    con.commit()\n",
    "                    \n",
    "                #increase value of current page\n",
    "                sql = \"\"\"\n",
    "                UPDATE worldwide_division_pages SET curr_page = {} WHERE division_id = {};\n",
    "                \"\"\".format(\n",
    "                    div[1] + 1,\n",
    "                    div[0]\n",
    "                )\n",
    "                cur.execute(sql)\n",
    "                con.commit()\n",
    "        finally:\n",
    "            if con:\n",
    "                con.close()\n",
    "        \n",
    "        #increase local value of current page\n",
    "        div[1] += 1\n",
    "\n",
    "#close browser\n",
    "driver.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
