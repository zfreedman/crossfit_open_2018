{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# data collection\n",
    "This notebook is responsible for collecting CrossFit 2018 Open Leaderboard data and athelete profile data *as represented at the time of data collection*."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## imports\n",
    "The below is just a set of import statements required to run the code in this notebook. A description for the purpose of each import statement should be commented above it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#sql connector\n",
    "import pymysql as pms\n",
    "#time recording and sleeping\n",
    "from time import time, sleep\n",
    "#browser automation\n",
    "from selenium import webdriver\n",
    "#from selenium.webdriver.common.by import By\n",
    "#from selenium.webdriver.support.ui import WebDriverWait\n",
    "#from selenium.webdriver.support import expected_conditions as EC"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## database credentials\n",
    "In order to connect to a local MySQL database, the block below runs to read the username, password, database name, and host required to establish a connection."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "db_user = \"\"\n",
    "db_pass = \"\"\n",
    "db_name = \"\"\n",
    "db_host = \"\"\n",
    "with open(\"database_credentials.txt\") as f:\n",
    "    db_user = f.readline().strip()\n",
    "    db_pass = f.readline().strip()\n",
    "    db_name = f.readline().strip()\n",
    "    db_host = f.readline().strip()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## test database connection\n",
    "This short snippet is going to attempt to connect to the database and drop out without doing anything. This is just to make sure the credentials and PyMySQL are working properly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_connect():\n",
    "    \"\"\"\n",
    "    Returns a database connection object using the default params\n",
    "    specified in the database_credentials file.\n",
    "    \"\"\"\n",
    "    return pms.connect(host=db_host, user=db_user, passwd=db_pass, db=db_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Connected.\n",
      "Got database cursor. Can make queries within here.\n",
      "Connection closed.\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    con = get_connect()\n",
    "    print(\"Connected.\")\n",
    "    with con.cursor() as cur:\n",
    "        print(\"Got database cursor. Can make queries within here.\")\n",
    "finally:\n",
    "    if con:\n",
    "        con.close()\n",
    "        print(\"Connection closed.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## urls\n",
    "These urls/variables can be used to jump to pages where leaderboard data is available."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "default_url = \"https://games.crossfit.com/leaderboard/open/2018?division=1&region=0&scaled=0&sort=0&occupation=0&page=1\"\n",
    "custom_url = \"https://games.crossfit.com/leaderboard/open/2018?division={}&region={}&scaled={}&sort={}&occupation={}&page={}\"\n",
    "\n",
    "#this map would be used to substitute values into the\n",
    "#custom url string at position \"division={}\" in place\n",
    "#of the \"{}\"\n",
    "map_division = {\n",
    "    \"men\": 1,\n",
    "    \"women\": 2,\n",
    "    \"team\": 11,\n",
    "    #men aged (35-39) inclusive\n",
    "    \"m35-39\": 18,\n",
    "    #women aged (35-39) inclusive\n",
    "    \"w35-39\": 19,\n",
    "    \"m40-44\": 12,\n",
    "    \"w40-44\": 13,\n",
    "    \"m45-49\": 3,\n",
    "    \"w45-49\": 4,\n",
    "    \"m50-54\": 5,\n",
    "    \"w50-54\": 6,\n",
    "    \"m55-59\": 7,\n",
    "    \"w55-59\": 8,\n",
    "    \"m60+\": 9,\n",
    "    \"w60+\": 10,\n",
    "    #boys aged (16-17) inclusive\n",
    "    \"b16-17\": 16,\n",
    "    #girls aged (16-17) inclusive\n",
    "    \"g16-17\": 17,\n",
    "    \"b14-15\": 14,\n",
    "    \"g14-15\": 15\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## getting custom url values\n",
    "Although the URL custom attributes can be harded, the more robust solution is to write a browser automation step, prior to the main data collection, that acquires the custom url attributes corresponding to each filter. This is done below.\n",
    "\n",
    "The goal is to obtain all of the maps like the one above in a more robust manner.\n",
    "\n",
    "Also, in the below step, it's important to note that the year and competition can also be used to filter results. However, at the current time, regional data is not available for 2018 (the open just finished). Furthermore, previous open leaderboard have different HTML structure (would require additional scraping code), and I really only care about 2018. Additionally, Rx'd/scaled, per-workout, occupation, and region are also available filtering criteria.\n",
    "\n",
    "**Rx'd/scaled and occupation**\n",
    "I don't care about these for the time being. This repo will only consider non-specific occupation and Rx'd athletes.\n",
    "\n",
    "**per-workout and region**\n",
    "I'll be able to do this filtering on my own (hypothetically). In order to do so, the region and per-workout scores will be scraped from the leaderboard and filtered on later in other notebooks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "HTML ids which will be used for filtering:\n",
      "['control-division', 'control-region']\n"
     ]
    }
   ],
   "source": [
    "filter_ids = [\"division\", \"region\"]\n",
    "for i in range(len(filter_ids)):\n",
    "    filter_ids[i] = \"control-\" + filter_ids[i]\n",
    "print(\"HTML ids which will be used for filtering:\\n{}\".format(filter_ids))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below we're going to go to the leaderboard (autonomously) and scrape the IDs [CrossFit](https://games.crossfit.com/leaderboard/open/2018?division=1&region=0&scaled=0&sort=0&occupation=0&page=1) uses for divisions and regions. Although I don't need to use the same IDs, it can only help to use the same mappings. When we're talking about IDs here, I mean the numeric values that would be used to plug into the `\"{}\"` occurences in the `custom_url` string above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==== region results(0) ====\n",
      "\tid, region\n",
      "==== division results(0) ====\n",
      "\tid, division\n",
      "scrape\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\unknown\\AppData\\Local\\conda\\conda\\envs\\crossfit_open_2018\\lib\\site-packages\\pymysql\\cursors.py:103: Warning: (1050, \"Table 'division' already exists\")\n",
      "  return self._nextset(False)\n"
     ]
    }
   ],
   "source": [
    "#attempt database connect\n",
    "try:\n",
    "    con = get_connect()\n",
    "    with con.cursor() as cur:\n",
    "        #create division and region tables if they don't exist\n",
    "        sql = \"\"\"\n",
    "        CREATE TABLE IF NOT EXISTS region (\n",
    "            id INT PRIMARY KEY,\n",
    "            region VARCHAR(24) NOT NULL\n",
    "        );\n",
    "        CREATE TABLE IF NOT EXISTS division (\n",
    "            id INT PRIMARY KEY,\n",
    "            division VARCHAR(16) NOT NULL\n",
    "        );\n",
    "        \"\"\"\n",
    "        cur.execute(sql)\n",
    "        \n",
    "        #attempt to get regions and divisions\n",
    "        result_counts = [-1, -1]\n",
    "        tables = [\"region\", \"division\"]\n",
    "        sql = \"\"\"\n",
    "        SELECT * FROM {};\n",
    "        \"\"\"\n",
    "        for i in range(len(tables)):\n",
    "            cur.execute(sql.format(tables[i]))\n",
    "            result = cur.fetchall()\n",
    "            #store number of results\n",
    "            result_counts[i] = len(result)\n",
    "            #output results\n",
    "            print(\"==== {} results({}) ====\".format(tables[i], result_counts[i]))\n",
    "            print(\"\\t\" + \", \".join([col[0] for col in cur.description]))\n",
    "            for j in range(result_counts[i]):\n",
    "                print(\"\\t{}: {}\").format(j, result[j])\n",
    "        \n",
    "        #if results for both are not empty, the values have\n",
    "        #already been scraped, so skip this\n",
    "        if result_counts[0] == 0 or result_counts[1] == 0:\n",
    "            \n",
    "        \n",
    "        #if either result is empty, rescrape both\n",
    "finally:\n",
    "    if con:\n",
    "        con.close()\n",
    "\n",
    "#spin up browser\n",
    "#driver = webdriver.Chrome()\n",
    "#driver.get(default_url)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
