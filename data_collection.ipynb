{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# data collection\n",
    "This notebook is responsible for collecting CrossFit 2018 Open Leaderboard data and athelete profile data *as represented at the time of data collection*."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## imports\n",
    "The below is just a set of import statements required to run the code in this notebook. A description for the purpose of each import statement should be commented above it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#sql connector\n",
    "import pymysql as pms\n",
    "#time recording and sleeping\n",
    "from time import time, sleep\n",
    "#browser automation\n",
    "from selenium import webdriver\n",
    "#from selenium.webdriver.common.by import By\n",
    "#from selenium.webdriver.support.ui import WebDriverWait\n",
    "#from selenium.webdriver.support import expected_conditions as EC"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## database credentials\n",
    "In order to connect to a local MySQL database, the block below runs to read the username, password, database name, and host required to establish a connection."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "db_user = \"\"\n",
    "db_pass = \"\"\n",
    "db_name = \"\"\n",
    "db_host = \"\"\n",
    "with open(\"database_credentials.txt\") as f:\n",
    "    db_user = f.readline().strip()\n",
    "    db_pass = f.readline().strip()\n",
    "    db_name = f.readline().strip()\n",
    "    db_host = f.readline().strip()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## test database connection\n",
    "This short snippet is going to attempt to connect to the database and drop out without doing anything. This is just to make sure the credentials and PyMySQL are working properly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_connect():\n",
    "    \"\"\"\n",
    "    Returns a database connection object using the default params\n",
    "    specified in the database_credentials file.\n",
    "    \"\"\"\n",
    "    return pms.connect(host=db_host, user=db_user, passwd=db_pass, db=db_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Connected.\n",
      "Got database cursor. Can make queries within here.\n",
      "Connection closed.\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    con = get_connect()\n",
    "    print(\"Connected.\")\n",
    "    with con.cursor() as cur:\n",
    "        print(\"Got database cursor. Can make queries within here.\")\n",
    "finally:\n",
    "    if con:\n",
    "        con.close()\n",
    "        print(\"Connection closed.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## urls\n",
    "These urls/variables can be used to jump to pages where leaderboard data is available."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "default_url = \"https://games.crossfit.com/leaderboard/open/2018?division=1&region=0&scaled=0&sort=0&occupation=0&page=1\"\n",
    "custom_url = \"https://games.crossfit.com/leaderboard/open/2018?division={}&region={}&scaled={}&sort={}&occupation={}&page={}\"\n",
    "\n",
    "#this map would be used to substitute values into the\n",
    "#custom url string at position \"division={}\" in place\n",
    "#of the \"{}\"\n",
    "map_division = {\n",
    "    \"men\": 1,\n",
    "    \"women\": 2,\n",
    "    \"team\": 11,\n",
    "    #men aged (35-39) inclusive\n",
    "    \"m35-39\": 18,\n",
    "    #women aged (35-39) inclusive\n",
    "    \"w35-39\": 19,\n",
    "    \"m40-44\": 12,\n",
    "    \"w40-44\": 13,\n",
    "    \"m45-49\": 3,\n",
    "    \"w45-49\": 4,\n",
    "    \"m50-54\": 5,\n",
    "    \"w50-54\": 6,\n",
    "    \"m55-59\": 7,\n",
    "    \"w55-59\": 8,\n",
    "    \"m60+\": 9,\n",
    "    \"w60+\": 10,\n",
    "    #boys aged (16-17) inclusive\n",
    "    \"b16-17\": 16,\n",
    "    #girls aged (16-17) inclusive\n",
    "    \"g16-17\": 17,\n",
    "    \"b14-15\": 14,\n",
    "    \"g14-15\": 15\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## getting custom url values\n",
    "Although the URL custom attributes can be harded, the more robust solution is to write a browser automation step, prior to the main data collection, that acquires the custom url attributes corresponding to each filter. This is done below.\n",
    "\n",
    "The goal is to obtain all of the maps like the one above in a more robust manner.\n",
    "\n",
    "Also, in the below step, it's important to note that the year and competition can also be used to filter results. However, at the current time, regional data is not available for 2018 (the open just finished). Furthermore, previous open leaderboard have different HTML structure (would require additional scraping code), and I really only care about 2018. Additionally, Rx'd/scaled, per-workout, occupation, and region are also available filtering criteria.\n",
    "\n",
    "**Rx'd/scaled and occupation**\n",
    "I don't care about these for the time being. This repo will only consider non-specific occupation and Rx'd athletes.\n",
    "\n",
    "**per-workout and region**\n",
    "I'll be able to do this filtering on my own (hypothetically). In order to do so, the region and per-workout scores will be scraped from the leaderboard and filtered on later in other notebooks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "HTML filters which will be used for filtering:\n",
      "['division', 'region']\n"
     ]
    }
   ],
   "source": [
    "filters = [\"division\", \"region\"]\n",
    "print(\"HTML filters which will be used for filtering:\\n{}\".format(filters))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below we're going to go to the leaderboard (autonomously) and scrape the IDs [CrossFit](https://games.crossfit.com/leaderboard/open/2018?division=1&region=0&scaled=0&sort=0&occupation=0&page=1) uses for divisions and regions. Although I don't need to use the same IDs, it can only help to use the same mappings. When we're talking about IDs here, I mean the numeric values that would be used to plug into the `\"{}\"` occurences in the `custom_url` string above.\n",
    "\n",
    "Here's the documentation I use for [Selenium](http://selenium-python.readthedocs.io/locating-elements.html)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==== division results(19) ====\n",
      "id, division\n",
      "0: (1, 'Men')\n",
      "1: (2, 'Women')\n",
      "2: (3, 'Men (45-49)')\n",
      "3: (4, 'Women (45-49)')\n",
      "4: (5, 'Men (50-54)')\n",
      "5: (6, 'Women (50-54)')\n",
      "6: (7, 'Men (55-59)')\n",
      "7: (8, 'Women (55-59)')\n",
      "8: (9, 'Men (60+)')\n",
      "9: (10, 'Women (60+)')\n",
      "10: (11, 'Team')\n",
      "11: (12, 'Men (40-44)')\n",
      "12: (13, 'Women (40-44)')\n",
      "13: (14, 'Boys (14-15)')\n",
      "14: (15, 'Girls (14-15)')\n",
      "15: (16, 'Boys (16-17)')\n",
      "16: (17, 'Girls (16-17)')\n",
      "17: (18, 'Men (35-39)')\n",
      "18: (19, 'Women (35-39)')\n",
      "==== region results(19) ====\n",
      "id, region\n",
      "0: (0, 'Worldwide')\n",
      "1: (5, 'Canada West')\n",
      "2: (6, 'Central East')\n",
      "3: (9, 'Mid Atlantic')\n",
      "4: (10, 'North Central')\n",
      "5: (11, 'North East')\n",
      "6: (14, 'South Central')\n",
      "7: (15, 'South East')\n",
      "8: (17, 'South West')\n",
      "9: (18, 'Canada East')\n",
      "10: (19, 'West Coast')\n",
      "11: (20, 'Asia')\n",
      "12: (21, 'Australasia')\n",
      "13: (22, 'Europe North')\n",
      "14: (23, 'Europe Central')\n",
      "15: (24, 'Europe South')\n",
      "16: (25, 'Africa Middle East')\n",
      "17: (26, 'Central America')\n",
      "18: (27, 'South America')\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\unknown\\AppData\\Local\\conda\\conda\\envs\\crossfit_open_2018\\lib\\site-packages\\pymysql\\cursors.py:103: Warning: (1050, \"Table 'division' already exists\")\n",
      "  return self._nextset(False)\n"
     ]
    }
   ],
   "source": [
    "#attempt database connect\n",
    "try:\n",
    "    con = get_connect()\n",
    "    with con.cursor() as cur:\n",
    "        #create division and region tables if they don't exist\n",
    "        sql = \"\"\"\n",
    "        CREATE TABLE IF NOT EXISTS region (\n",
    "            id INT PRIMARY KEY,\n",
    "            region VARCHAR(24) NOT NULL\n",
    "        );\n",
    "        CREATE TABLE IF NOT EXISTS division (\n",
    "            id INT PRIMARY KEY,\n",
    "            division VARCHAR(16) NOT NULL\n",
    "        );\n",
    "        \"\"\"\n",
    "        cur.execute(sql)\n",
    "        \n",
    "        #attempt to get regions and divisions\n",
    "        result_counts = [-1, -1]\n",
    "        sql = \"\"\"\n",
    "        SELECT * FROM {};\n",
    "        \"\"\"\n",
    "        for i in range(len(filters)):\n",
    "            cur.execute(sql.format(filters[i]))\n",
    "            result = cur.fetchall()\n",
    "            #store number of results\n",
    "            result_counts[i] = len(result)\n",
    "            #output results\n",
    "            print(\"==== {} results({}) ====\".format(filters[i], result_counts[i]))\n",
    "            print(\", \".join([col[0] for col in cur.description]))\n",
    "            for j in range(result_counts[i]):\n",
    "                print(\"{}: {}\".format(j, result[j]))\n",
    "        \n",
    "        #if results for both are not empty, the values have\n",
    "        #already been scraped, so skip this\n",
    "        if result_counts[0] < 1 or result_counts[1] < 1:\n",
    "            #Store id, region/div pairs as tuples (id_0, region_0/div_0)\n",
    "            #the below entries will have 2 lists of such tuples, 1 for\n",
    "            #each filter_id\n",
    "            entries = []\n",
    "            #spin up browser\n",
    "            driver = webdriver.Chrome()\n",
    "            driver.get(default_url)\n",
    "            \n",
    "            #iterate over useful filters\n",
    "            for i in range(len(filters)):\n",
    "                #ids are formatted with control- as a prefix\n",
    "                dropdown = driver.find_element_by_id(\"control-\" + filters[i])\n",
    "                options = dropdown.find_elements_by_tag_name(\"option\")\n",
    "                #aggregate entries\n",
    "                #also, this is making 2 calls to o.get_attribute(\"value\"), and this\n",
    "                #could be done \"more efficiently\" without the list comprehension\n",
    "                entries.append([(int(o.get_attribute(\"value\")), o.get_attribute(\"innerText\"))\n",
    "                            for o in options if o.get_attribute(\"value\") != \"\"])\n",
    "                #print(entries[i])\n",
    "            \n",
    "            #close driver\n",
    "            driver.close()\n",
    "        \n",
    "            #write entries to file\n",
    "            sql = \"\"\"\n",
    "            INSERT INTO {}(id, {}) VALUES\n",
    "                {}\n",
    "            \"\"\"\n",
    "            for i in range(len(filters)):\n",
    "                cur.execute(sql.format(filters[i], filters[i],\n",
    "                                       \",\\n\".join([\"({}, '{}')\".format(e[0], e[1]) for e in entries[i]])))\n",
    "            #commit inserts\n",
    "            con.commit()\n",
    "finally:\n",
    "    if con:\n",
    "        con.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## checkout filter values\n",
    "Now that we've ensured the filters are in the database, let's grab them and create mappings. These will be necessary for the athlete table."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#this map will contain maps for each filter\n",
    "filter_maps = {}\n",
    "try:\n",
    "    con = get_connect()\n",
    "    with con.cursor() as cur:\n",
    "        sql = \"\"\"\n",
    "        SELECT * FROM {};\n",
    "        \"\"\"\n",
    "        for f in filters:\n",
    "            #add filter map\n",
    "            filter_maps[f] = {}\n",
    "            #get results\n",
    "            cur.execute(sql.format(f))\n",
    "            result = cur.fetchall()\n",
    "            #store results\n",
    "            for r in result:\n",
    "                #create mapping from div/region -> id\n",
    "                filter_maps[f][r[1]] = r[0]\n",
    "finally:\n",
    "    if con:\n",
    "        con.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## hard-coded values\n",
    "At the time of data collection, the 2018 CrossFit Open has ended. With this, we're making the assumption the total number of pages for each leaderboard will not change (this is not necessarily 100%, but I'm assuming it's very close to 100%). The leaderboard is structured in a way such that if a page is requested beyond the total number of leaderboard pages available for a specific filter, it redirects back to the first leaderboard page.\n",
    "\n",
    "Although there are different ways to handle this, in order to know when to stop scraping for a specific filter, we'll just scrape the index of the last available page, which is available at the bottom of each leaderboard. Additionally, the current index for each filter will also scraped, defaulting to the first leaderboard page: 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\unknown\\AppData\\Local\\conda\\conda\\envs\\crossfit_open_2018\\lib\\site-packages\\pymysql\\cursors.py:166: Warning: (1050, \"Table 'worldwide_division_pages' already exists\")\n",
      "  result = self._query(query)\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    con = get_connect()\n",
    "    with con.cursor() as cur:\n",
    "        #create database to store these worldwide-division indices\n",
    "        #(could easily be stored in a flat file instead)\n",
    "        sql = \"\"\"\n",
    "        CREATE TABLE IF NOT EXISTS worldwide_division_pages (\n",
    "            division_id INT PRIMARY KEY,\n",
    "            FOREIGN KEY (division_id)\n",
    "                REFERENCES division(id)\n",
    "                ON DELETE CASCADE,\n",
    "            curr_page INT NOT NULL DEFAULT 1,\n",
    "            last_page INT NOT NULL DEFAULT -1\n",
    "        )\n",
    "        \"\"\"\n",
    "        cur.execute(sql)\n",
    "        \n",
    "        #check if table has been populated already in a previous run\n",
    "        sql = \"\"\"\n",
    "        SELECT * FROM worldwide_division_pages;\n",
    "        \"\"\"\n",
    "        cur.execute(sql)\n",
    "        result = cur.fetchall()\n",
    "        if len(result) == 0:\n",
    "            #cross-populate division id's from division table\n",
    "            sql = \"\"\"\n",
    "            INSERT INTO worldwide_division_pages (division_id)\n",
    "                SELECT id FROM division;\n",
    "            \"\"\"\n",
    "            cur.execute(sql)\n",
    "            con.commit()\n",
    "            \n",
    "            #grab option IDs from database\n",
    "            #this could be done by just collecting them from the browser, the values\n",
    "            #are supposed to be identical\n",
    "            sql = \"\"\"\n",
    "            SELECT division_id FROM worldwide_division_pages;\n",
    "            \"\"\"\n",
    "            cur.execute(sql)\n",
    "            result = cur.fetchall()\n",
    "            ids = [r[0] for r in result]\n",
    "            print(ids)\n",
    "                     \n",
    "            #scrape last_page values for each division\n",
    "            driver = webdriver.Chrome()\n",
    "            driver.get(default_url)\n",
    "            \n",
    "            #grab division selectable and store in browser\n",
    "            inject_store_select = \"\"\"\n",
    "            window.division_select = document.getElementById(\"control-division\");\n",
    "            \"\"\"\n",
    "            driver.execute_script(inject_store_select)\n",
    "            \n",
    "            last_pages = []\n",
    "            #iterate over ids\n",
    "            for i in ids:\n",
    "                #force select element to change to new dropdown option\n",
    "                inject_change_select = \"\"\"\n",
    "                window.division_select.value = {};\n",
    "                window.division_select.dispatchEvent(new Event(\"change\"));\n",
    "                \"\"\".format(i)\n",
    "                driver.execute_script(inject_change_select)\n",
    "                #wait for page to update\n",
    "                sleep(2)\n",
    "                #grab last page text from the bottom\n",
    "                last_pages.append(int(\n",
    "                    driver.find_element_by_class_name(\"nums\")\n",
    "                        .find_elements_by_tag_name(\"a\")[-1]\n",
    "                        .get_attribute(\"innerText\")\n",
    "                ))\n",
    "            \n",
    "            #write updates to file in bulk\n",
    "            sql = \"\"\"\n",
    "            INSERT INTO worldwide_division_pages(division_id, last_page) VALUES\n",
    "                {}\n",
    "                ON DUPLICATE KEY UPDATE last_page = VALUES(last_page);\n",
    "            \"\"\"\n",
    "            cur.execute(\n",
    "                sql.format(\n",
    "                    \",\\n\".join([\"({}, '{}')\".format(ids[i], last_pages[i]) for i in range(len(ids))])\n",
    "                )\n",
    "            )\n",
    "            con.commit()\n",
    "            \n",
    "            #close driver\n",
    "            driver.close()\n",
    "finally:\n",
    "    if con:\n",
    "        con.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### additional fixed filters\n",
    "Below are the hard-coded values for scaled, occupation, sort, and workout type."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "region = 0\n",
    "scaled = 0\n",
    "sort = 0\n",
    "occupation = 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## leaderboard/athlete scraping\n",
    "Scraping the Open leaderboard data is only half of the data required per athlete. The remaining data will be collected from their athlete profile, containing statistics for their Back Squat, Fran, and other CrossFit staples. This will be done **after** the Open data for a specific division is completely finished. Therefore, the scraping process pipeline from this point forward is as follows:\n",
    "* for each division:\n",
    "    * scrape all leaderboard data\n",
    "    * for each athlete:\n",
    "        * scrape profile data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### reading in the pages to be scraped from the Open leaderboard\n",
    "Below the pages left to be scraped will be collected from the database and sorted in order from least athletes to most athletes **per division**. Divisions which have complete Open leaderboard data already scraped will have a current page value exceeding the last page value by 1 (`curr_page = 3`, `last_page = 2`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    con = get_connect()\n",
    "    with con.cursor() as cur:\n",
    "        sql = \"\"\"\n",
    "        SELECT * FROM worldwide_division_pages;\n",
    "        \"\"\"\n",
    "        cur.execute(sql)\n",
    "        #sort results based on last_page value\n",
    "        division_pages = sorted(cur.fetchall(), key=lambda tup: tup[2])\n",
    "finally:\n",
    "    if con:\n",
    "        con.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(15, 1, 32),\n",
       " (14, 1, 40),\n",
       " (10, 1, 45),\n",
       " (17, 1, 47),\n",
       " (9, 1, 55),\n",
       " (16, 1, 63),\n",
       " (11, 1, 74),\n",
       " (8, 1, 76),\n",
       " (7, 1, 91),\n",
       " (6, 1, 124),\n",
       " (5, 1, 167),\n",
       " (4, 1, 239),\n",
       " (3, 1, 337),\n",
       " (13, 1, 398),\n",
       " (12, 1, 568),\n",
       " (19, 1, 619),\n",
       " (18, 1, 894),\n",
       " (2, 1, 3441),\n",
       " (1, 1, 4552)]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "division_pages"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
