{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# data collection\n",
    "This notebook is responsible for collecting CrossFit 2018 Open Leaderboard data and athelete profile data *as represented at the time of data collection*."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## imports\n",
    "The below is just a set of import statements required to run the code in this notebook. A description for the purpose of each import statement should be commented above it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#sql connector\n",
    "import pymysql as pms\n",
    "#time recording and sleeping\n",
    "from time import time, sleep\n",
    "#browser automation\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## database credentials\n",
    "In order to connect to a local MySQL database, the block below runs to read the username, password, database name, and host required to establish a connection."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "db_user = \"\"\n",
    "db_pass = \"\"\n",
    "db_name = \"\"\n",
    "db_host = \"\"\n",
    "with open(\"database_credentials2.txt\") as f:\n",
    "    db_user = f.readline().strip()\n",
    "    db_pass = f.readline().strip()\n",
    "    db_name = f.readline().strip()\n",
    "    db_host = f.readline().strip()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## test database connection\n",
    "This short snippet is going to attempt to connect to the database and drop out without doing anything. This is just to make sure the credentials and PyMySQL are working properly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_connect():\n",
    "    \"\"\"\n",
    "    Returns a database connection object using the default params\n",
    "    specified in the database_credentials file.\n",
    "    \"\"\"\n",
    "    return pms.connect(host=db_host, user=db_user, passwd=db_pass, db=db_name, charset=\"utf8\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Connected.\n",
      "Got database cursor. Can make queries within here.\n",
      "Connection closed.\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    con = get_connect()\n",
    "    print(\"Connected.\")\n",
    "    with con.cursor() as cur:\n",
    "        print(\"Got database cursor. Can make queries within here.\")\n",
    "finally:\n",
    "    if con:\n",
    "        con.close()\n",
    "        print(\"Connection closed.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## urls\n",
    "These urls/variables can be used to jump to pages where leaderboard data is available."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "default_url = \"https://games.crossfit.com/leaderboard/open/2018?division=1&region=0&scaled=0&sort=0&occupation=0&page=1\"\n",
    "custom_url = \"https://games.crossfit.com/leaderboard/open/2018?division={}&region={}&scaled={}&sort={}&occupation={}&page={}\"\n",
    "athlete_url = \"https://games.crossfit.com/athlete/{}\"\n",
    "\n",
    "#this map would be used to substitute values into the\n",
    "#custom url string at position \"division={}\" in place\n",
    "#of the \"{}\"\n",
    "map_division = {\n",
    "    \"men\": 1,\n",
    "    \"women\": 2,\n",
    "    \"team\": 11,\n",
    "    #men aged (35-39) inclusive\n",
    "    \"m35-39\": 18,\n",
    "    #women aged (35-39) inclusive\n",
    "    \"w35-39\": 19,\n",
    "    \"m40-44\": 12,\n",
    "    \"w40-44\": 13,\n",
    "    \"m45-49\": 3,\n",
    "    \"w45-49\": 4,\n",
    "    \"m50-54\": 5,\n",
    "    \"w50-54\": 6,\n",
    "    \"m55-59\": 7,\n",
    "    \"w55-59\": 8,\n",
    "    \"m60+\": 9,\n",
    "    \"w60+\": 10,\n",
    "    #boys aged (16-17) inclusive\n",
    "    \"b16-17\": 16,\n",
    "    #girls aged (16-17) inclusive\n",
    "    \"g16-17\": 17,\n",
    "    \"b14-15\": 14,\n",
    "    \"g14-15\": 15\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## getting custom url values\n",
    "Although the URL custom attributes can be harded, the more robust solution is to write a browser automation step, prior to the main data collection, that acquires the custom url attributes corresponding to each filter. This is done below.\n",
    "\n",
    "The goal is to obtain all of the maps like the one above in a more robust manner.\n",
    "\n",
    "Also, in the below step, it's important to note that the year and competition can also be used to filter results. However, at the current time, regional data is not available for 2018 (the open just finished). Furthermore, previous open leaderboard have different HTML structure (would require additional scraping code), and I really only care about 2018. Additionally, Rx'd/scaled, per-workout, occupation, and region are also available filtering criteria.\n",
    "\n",
    "**Rx'd/scaled and occupation**\n",
    "I don't care about these for the time being. This repo will only consider non-specific occupation and Rx'd athletes.\n",
    "\n",
    "**per-workout and region**\n",
    "I'll be able to do this filtering on my own (hypothetically). In order to do so, the region and per-workout scores will be scraped from the leaderboard and filtered on later in other notebooks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "HTML filters which will be used for filtering:\n",
      "['division', 'region']\n"
     ]
    }
   ],
   "source": [
    "filters = [\"division\", \"region\"]\n",
    "print(\"HTML filters which will be used for filtering:\\n{}\".format(filters))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below we're going to go to the leaderboard (autonomously) and scrape the IDs [CrossFit](https://games.crossfit.com/leaderboard/open/2018?division=1&region=0&scaled=0&sort=0&occupation=0&page=1) uses for divisions and regions. Although I don't need to use the same IDs, it can only help to use the same mappings. When we're talking about IDs here, I mean the numeric values that would be used to plug into the `\"{}\"` occurences in the `custom_url` string above.\n",
    "\n",
    "Here's the documentation I use for [Selenium](http://selenium-python.readthedocs.io/locating-elements.html)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\unknown\\AppData\\Local\\conda\\conda\\envs\\crossfit_open_2018\\lib\\site-packages\\pymysql\\cursors.py:103: Warning: (1050, \"Table 'division' already exists\")\n",
      "  return self._nextset(False)\n"
     ]
    }
   ],
   "source": [
    "#attempt database connect\n",
    "try:\n",
    "    con = get_connect()\n",
    "    with con.cursor() as cur:\n",
    "        #create division and region tables if they don't exist\n",
    "        sql = \"\"\"\n",
    "        CREATE TABLE IF NOT EXISTS region (\n",
    "            id INT PRIMARY KEY,\n",
    "            region VARCHAR(24) NOT NULL\n",
    "        );\n",
    "        CREATE TABLE IF NOT EXISTS division (\n",
    "            id INT PRIMARY KEY,\n",
    "            division VARCHAR(16) NOT NULL\n",
    "        );\n",
    "        \"\"\"\n",
    "        cur.execute(sql)\n",
    "        \n",
    "        #attempt to get regions and divisions\n",
    "        result_counts = [-1, -1]\n",
    "        sql = \"\"\"\n",
    "        SELECT * FROM {};\n",
    "        \"\"\"\n",
    "        for i in range(len(filters)):\n",
    "            cur.execute(sql.format(filters[i]))\n",
    "            result = cur.fetchall()\n",
    "            #store number of results\n",
    "            result_counts[i] = len(result)\n",
    "            #output results\n",
    "            #print(\"==== {} results({}) ====\".format(filters[i], result_counts[i]))\n",
    "            #print(\", \".join([col[0] for col in cur.description]))\n",
    "            for j in range(result_counts[i]):\n",
    "                break\n",
    "                #print(\"{}: {}\".format(j, result[j]))\n",
    "        \n",
    "        #if results for both are not empty, the values have\n",
    "        #already been scraped, so skip this\n",
    "        if result_counts[0] < 1 or result_counts[1] < 1:\n",
    "            #Store id, region/div pairs as tuples (id_0, region_0/div_0)\n",
    "            #the below entries will have 2 lists of such tuples, 1 for\n",
    "            #each filter_id\n",
    "            entries = []\n",
    "            #spin up browser\n",
    "            driver = webdriver.Chrome()\n",
    "            driver.get(default_url)\n",
    "            \n",
    "            #iterate over useful filters\n",
    "            for i in range(len(filters)):\n",
    "                #ids are formatted with control- as a prefix\n",
    "                dropdown = driver.find_element_by_id(\"control-\" + filters[i])\n",
    "                options = dropdown.find_elements_by_tag_name(\"option\")\n",
    "                #aggregate entries\n",
    "                #also, this is making 2 calls to o.get_attribute(\"value\"), and this\n",
    "                #could be done \"more efficiently\" without the list comprehension\n",
    "                entries.append([(int(o.get_attribute(\"value\")), o.get_attribute(\"innerText\"))\n",
    "                            for o in options if o.get_attribute(\"value\") != \"\"])\n",
    "                #print(entries[i])\n",
    "            \n",
    "            #close driver\n",
    "            driver.close()\n",
    "        \n",
    "            #write entries to file\n",
    "            sql = \"\"\"\n",
    "            INSERT INTO {}(id, {}) VALUES\n",
    "                {}\n",
    "            \"\"\"\n",
    "            for i in range(len(filters)):\n",
    "                cur.execute(sql.format(filters[i], filters[i],\n",
    "                                       \",\\n\".join([\"({}, '{}')\".format(e[0], e[1]) for e in entries[i]])))\n",
    "            #commit inserts\n",
    "            con.commit()\n",
    "finally:\n",
    "    if con:\n",
    "        con.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "try:\n",
    "    con = get_connect()\n",
    "    with con.cursor() as cur:\n",
    "        #create database to store these worldwide-division indices\n",
    "        #(could easily be stored in a flat file instead)\n",
    "        sql = \"\"\"\n",
    "            DELETE FROM division WHERE id=11;\n",
    "        \"\"\"\n",
    "        cur.execute(sql)\n",
    "        con.commit()\n",
    "finally:\n",
    "    if con:\n",
    "        con.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## checkout filter values\n",
    "Now that we've ensured the filters are in the database, let's grab them and create mappings. These will be necessary for the athlete table."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#this map will contain maps for each filter\n",
    "filter_maps = {}\n",
    "try:\n",
    "    con = get_connect()\n",
    "    with con.cursor() as cur:\n",
    "        sql = \"\"\"\n",
    "        SELECT * FROM {};\n",
    "        \"\"\"\n",
    "        for f in filters:\n",
    "            #add filter map\n",
    "            filter_maps[f] = {}\n",
    "            #get results\n",
    "            cur.execute(sql.format(f))\n",
    "            result = cur.fetchall()\n",
    "            #store results\n",
    "            for r in result:\n",
    "                #create mapping from div/region -> id or vice-versa (depending on scraping needs)\n",
    "                if f == \"division\":\n",
    "                    filter_maps[f][r[0]] = r[1]\n",
    "                elif f == \"region\":\n",
    "                    filter_maps[f][r[1]] = r[0]\n",
    "finally:\n",
    "    if con:\n",
    "        con.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#filter_maps"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## hard-coded values\n",
    "At the time of data collection, the 2018 CrossFit Open has ended. With this, we're making the assumption the total number of pages for each leaderboard will not change (this is not necessarily 100%, but I'm assuming it's very close to 100%). The leaderboard is structured in a way such that if a page is requested beyond the total number of leaderboard pages available for a specific filter, it redirects back to the first leaderboard page.\n",
    "\n",
    "Although there are different ways to handle this, in order to know when to stop scraping for a specific filter, we'll just scrape the index of the last available page, which is available at the bottom of each leaderboard. Additionally, the current index for each filter will also scraped, defaulting to the first leaderboard page: 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\unknown\\AppData\\Local\\conda\\conda\\envs\\crossfit_open_2018\\lib\\site-packages\\pymysql\\cursors.py:166: Warning: (1050, \"Table 'worldwide_division_pages' already exists\")\n",
      "  result = self._query(query)\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    con = get_connect()\n",
    "    with con.cursor() as cur:\n",
    "        #create database to store these worldwide-division indices\n",
    "        #(could easily be stored in a flat file instead)\n",
    "        sql = \"\"\"\n",
    "        CREATE TABLE IF NOT EXISTS worldwide_division_pages (\n",
    "            division_id INT PRIMARY KEY,\n",
    "            FOREIGN KEY (division_id)\n",
    "                REFERENCES division(id)\n",
    "                ON DELETE CASCADE,\n",
    "            curr_page INT NOT NULL DEFAULT 1,\n",
    "            last_page INT NOT NULL DEFAULT -1\n",
    "        )\n",
    "        \"\"\"\n",
    "        cur.execute(sql)\n",
    "        \n",
    "        #check if table has been populated already in a previous run\n",
    "        sql = \"\"\"\n",
    "        SELECT * FROM worldwide_division_pages;\n",
    "        \"\"\"\n",
    "        cur.execute(sql)\n",
    "        result = cur.fetchall()\n",
    "        if len(result) == 0:\n",
    "            #cross-populate division id's from division table\n",
    "            sql = \"\"\"\n",
    "            INSERT INTO worldwide_division_pages (division_id)\n",
    "                SELECT id FROM division;\n",
    "            \"\"\"\n",
    "            cur.execute(sql)\n",
    "            con.commit()\n",
    "            \n",
    "            #grab option IDs from database\n",
    "            #this could be done by just collecting them from the browser, the values\n",
    "            #are supposed to be identical\n",
    "            sql = \"\"\"\n",
    "            SELECT division_id FROM worldwide_division_pages;\n",
    "            \"\"\"\n",
    "            cur.execute(sql)\n",
    "            result = cur.fetchall()\n",
    "            ids = [r[0] for r in result]\n",
    "            print(ids)\n",
    "                     \n",
    "            #scrape last_page values for each division\n",
    "            driver = webdriver.Chrome()\n",
    "            driver.get(default_url)\n",
    "            \n",
    "            #grab division selectable and store in browser\n",
    "            inject_store_select = \"\"\"\n",
    "            window.division_select = document.getElementById(\"control-division\");\n",
    "            \"\"\"\n",
    "            driver.execute_script(inject_store_select)\n",
    "            \n",
    "            last_pages = []\n",
    "            #iterate over ids\n",
    "            for i in ids:\n",
    "                #force select element to change to new dropdown option\n",
    "                inject_change_select = \"\"\"\n",
    "                window.division_select.value = {};\n",
    "                window.division_select.dispatchEvent(new Event(\"change\"));\n",
    "                \"\"\".format(i)\n",
    "                driver.execute_script(inject_change_select)\n",
    "                #wait for page to update\n",
    "                sleep(2)\n",
    "                #grab last page text from the bottom\n",
    "                last_pages.append(int(\n",
    "                    driver.find_element_by_class_name(\"nums\")\n",
    "                        .find_elements_by_tag_name(\"a\")[-1]\n",
    "                        .get_attribute(\"innerText\")\n",
    "                ))\n",
    "            \n",
    "            #write updates to file in bulk\n",
    "            sql = \"\"\"\n",
    "            INSERT INTO worldwide_division_pages(division_id, last_page) VALUES\n",
    "                {}\n",
    "                ON DUPLICATE KEY UPDATE last_page = VALUES(last_page);\n",
    "            \"\"\"\n",
    "            cur.execute(\n",
    "                sql.format(\n",
    "                    \",\\n\".join([\"({}, '{}')\".format(ids[i], last_pages[i]) for i in range(len(ids))])\n",
    "                )\n",
    "            )\n",
    "            con.commit()\n",
    "            \n",
    "            #close driver\n",
    "            driver.close()\n",
    "finally:\n",
    "    if con:\n",
    "        con.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### deleting the teams entry\n",
    "The teams leaderboard/profile data is not fit for this learning task in it's current state, so we'll remove it from the `worldwide_division_pages` and `division` tables so we don't scrape any data for it."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### additional fixed filters\n",
    "Below are the hard-coded values for scaled, occupation, sort, and workout type."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "custom url:\n",
      "https://games.crossfit.com/leaderboard/open/2018?division={}&region=0&scaled=0&sort=0&occupation=0&page={}\n"
     ]
    }
   ],
   "source": [
    "region = 0\n",
    "scaled = 0\n",
    "sort = 0\n",
    "occupation = 0\n",
    "custom_url = (\n",
    "    \"https://games.crossfit.com/leaderboard/open/2018?division={}&region={}&scaled={}&sort={}&occupation={}&page={}\"\n",
    "        .format(\"{}\", region, scaled, sort, occupation, \"{}\")\n",
    ")\n",
    "print(\"custom url:\\n{}\".format(custom_url))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## leaderboard/athlete scraping overview\n",
    "Scraping the Open leaderboard data is only half of the data required per athlete. The remaining data will be collected from their athlete profile, containing statistics for their Back Squat, Fran, and other CrossFit staples. This will be done **after** the Open data for a specific division is completely finished. Therefore, the scraping process pipeline from this point forward is as follows:\n",
    "* for each division:\n",
    "    * for each leaderboard page:\n",
    "        * scrape all athlete leaderboard information\n",
    "        * for each athlete:\n",
    "            * scrape profile data\n",
    "        * write all athlete data to file\n",
    "        * update current page for this division"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### reading in the pages to be scraped from the Open leaderboard\n",
    "Below the pages left to be scraped will be collected from the database and sorted in order from least athletes to most athletes **per division**. Divisions which have complete Open leaderboard data already scraped will have a current page value exceeding the last page value by 1 (`curr_page = 3`, `last_page = 2`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "try:\n",
    "    con = get_connect()\n",
    "    with con.cursor() as cur:\n",
    "        sql = \"\"\"\n",
    "        SELECT * FROM worldwide_division_pages;\n",
    "        \"\"\"\n",
    "        cur.execute(sql)\n",
    "        #sort results based on last_page value\n",
    "        division_pages = sorted(\n",
    "            list(\n",
    "                map(\n",
    "                    lambda tup: list(tup),\n",
    "                    cur.fetchall()\n",
    "                )\n",
    "            ), \n",
    "            key=lambda tup: tup[2],\n",
    "            reverse=True\n",
    "        )\n",
    "finally:\n",
    "    if con:\n",
    "        con.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### filtering division pages\n",
    "Divisions that have already been completely scraped (`curr_page == last_page + 1`) should be excluded from the scraping process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original division pages:\n",
      "\t[1, 3, 4552]\n",
      "\t[2, 5, 3440]\n",
      "\t[18, 6, 894]\n",
      "\t[19, 2, 619]\n",
      "\t[12, 11, 568]\n",
      "\t[13, 3, 398]\n",
      "\t[3, 7, 337]\n",
      "\t[4, 6, 239]\n",
      "\t[5, 5, 167]\n",
      "\t[6, 5, 124]\n",
      "\t[7, 4, 91]\n",
      "\t[8, 11, 76]\n",
      "\t[16, 4, 63]\n",
      "\t[9, 5, 55]\n",
      "\t[17, 4, 47]\n",
      "\t[10, 3, 45]\n",
      "\t[14, 12, 40]\n",
      "\t[15, 8, 32]\n",
      "Filtered division pages:\n",
      "\t[1, 3, 4552]\n",
      "\t[2, 5, 3440]\n",
      "\t[18, 6, 894]\n",
      "\t[19, 2, 619]\n",
      "\t[12, 11, 568]\n",
      "\t[13, 3, 398]\n",
      "\t[3, 7, 337]\n",
      "\t[4, 6, 239]\n",
      "\t[5, 5, 167]\n",
      "\t[6, 5, 124]\n",
      "\t[7, 4, 91]\n",
      "\t[8, 11, 76]\n",
      "\t[16, 4, 63]\n",
      "\t[9, 5, 55]\n",
      "\t[17, 4, 47]\n",
      "\t[10, 3, 45]\n",
      "\t[14, 12, 40]\n",
      "\t[15, 8, 32]\n"
     ]
    }
   ],
   "source": [
    "#output\n",
    "print(\"Original division pages:\\n{}\".format(\"\\n\".join([\"\\t{}\".format(d) for d in division_pages])))\n",
    "#filter\n",
    "filtered_division_pages = [d for d in division_pages if d[1] <= d[2]]\n",
    "print(\"Filtered division pages:\\n{}\".format(\"\\n\".join([\"\\t{}\".format(d) for d in filtered_division_pages])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## setting up the athlete tables\n",
    "The athlete table will contain data for athletes from 2 sources: leaderboards and profiles. The data from each in it's native form is shown below.\n",
    "\n",
    "### leaderboards\n",
    "<img src=\"images/leaderboard.png\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### profiles\n",
    "<img src=\"images/basic_stats.png\" />\n",
    "<img src=\"images/benchmark_stats.png\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Although this data could be separated into 2 or 3 different tables, for the scope of the planned analysis, it can all go into the same table. The collected data per athlete will contain the following:\n",
    "* leaderboard\n",
    "    * name\n",
    "    * 18.1, 18.2, 18.2a, 18.3, 18.4, 18.5 scores (not rank, a.k.a. time or reps or weight)\n",
    "    * height, weight, age\n",
    "    * region, division\n",
    "* profile\n",
    "    * affiliate\n",
    "    * back squat, clean and jerk, snatch, deadlift\n",
    "    * fight gone bad, fran, grace, helen, filthy 50\n",
    "    * max pull-ups\n",
    "    * sprint 400m, run 5k\n",
    "    \n",
    "The table containing all this data is created below. Time data will be recorded in seconds, weight in pounds, height in inches, and age in years."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\unknown\\AppData\\Local\\conda\\conda\\envs\\crossfit_open_2018\\lib\\site-packages\\pymysql\\cursors.py:166: Warning: (1050, \"Table 'athlete' already exists\")\n",
      "  result = self._query(query)\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    con = get_connect()\n",
    "    with con.cursor() as cur:\n",
    "        sql = \"\"\"\n",
    "        CREATE TABLE IF NOT EXISTS athlete (\n",
    "            id INT PRIMARY KEY,\n",
    "            name VARCHAR(128) NOT NULL,\n",
    "            leaderboard_18_1_reps INT,\n",
    "            leaderboard_18_2_time_secs INT,\n",
    "            leaderboard_18_2a_weight_lbs INT,\n",
    "            leaderboard_18_3_time_secs INT,\n",
    "            leaderboard_18_4_time_secs INT,\n",
    "            leaderboard_18_5_reps INT,\n",
    "            \n",
    "            height_in INT,\n",
    "            weight_lbs INT,\n",
    "            age_years INT,\n",
    "            \n",
    "            region_id INT NOT NULL,\n",
    "            FOREIGN KEY (region_id)\n",
    "                REFERENCES region(id)\n",
    "                ON DELETE CASCADE,\n",
    "            division_id INT NOT NULL,\n",
    "            FOREIGN KEY (division_id)\n",
    "                REFERENCES division(id)\n",
    "                ON DELETE CASCADE,\n",
    "            \n",
    "            affiliate_id INT,\n",
    "            \n",
    "            back_squat_lbs INT,\n",
    "            clean_and_jerk_lbs INT,\n",
    "            snatch_lbs INT,\n",
    "            deadlift_lbs INT,\n",
    "            \n",
    "            fight_gone_bad_time_secs INT,\n",
    "            fran_time_secs INT,\n",
    "            grace_time_secs INT,\n",
    "            helen_time_secs INT,\n",
    "            filthy_50_time_secs INT,\n",
    "            \n",
    "            max_pull_ups INT,\n",
    "            \n",
    "            sprint_400_m_time_secs INT,\n",
    "            run_5_km_time_secs INT\n",
    "        );\n",
    "        \"\"\"\n",
    "        cur.execute(sql)\n",
    "finally:\n",
    "    if con:\n",
    "        con.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## scrape athlete data\n",
    "Here we go. glhf\n",
    "\n",
    "### Constants used during scraping\n",
    "The below constants are used during the scraping process. The uses are commented, but they range from CSS selectors, output formatting, SQL column names, and time multipliers (h:m:s)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#used for output styling\n",
    "center_space = 60\n",
    "center_sep = \" \"\n",
    "\n",
    "#for leaderboard workouts\n",
    "workout_keys = [\n",
    "    \"18_1_reps\", \"18_2_time_secs\", \"18_2a_weight_lbs\", \"18_3_time_secs\", \"18_4_time_secs\", \"18_5_reps\"]\n",
    "workout_keys = list(map(lambda k: \"leaderboard_\" + k, workout_keys))\n",
    "\n",
    "#for stats metrics on the athlete profile page\n",
    "stats_keys = [\n",
    "    \"back_squat_lbs\",\n",
    "    \"clean_and_jerk_lbs\",\n",
    "    \"snatch_lbs\",\n",
    "    \n",
    "    \"deadlift_lbs\",\n",
    "    \"fight_gone_bad_time_secs\",\n",
    "    \"max_pull_ups\",\n",
    "    \n",
    "    \"fran_time_secs\",\n",
    "    \"grace_time_secs\",\n",
    "    \"helen_time_secs\",\n",
    "    \n",
    "    \"filthy_50_time_secs\",\n",
    "    \"sprint_400_m_time_secs\",\n",
    "    \"run_5_km_time_secs\"\n",
    "]\n",
    "\n",
    "#for other values (height, weight, etc.)\n",
    "height_key = \"height_in\"\n",
    "weight_key = \"weight_lbs\"\n",
    "age_key = \"age_years\"\n",
    "id_key = \"id\"\n",
    "region_key = \"region_id\"\n",
    "division_key = \"division_id\"\n",
    "name_key = \"name\"\n",
    "affiliate_key = \"affiliate_id\"\n",
    "\n",
    "#all keys (used for insertion)\n",
    "all_keys = sorted(workout_keys + stats_keys + [\n",
    "    height_key, weight_key, age_key, id_key, region_key, division_key, name_key, affiliate_key\n",
    "])\n",
    "all_keys_str = \", \".join(all_keys)\n",
    "\n",
    "#time map values for converting to seconds\n",
    "time_mults = [3600, 60, 1]\n",
    "\n",
    "#used for getting statistics on athlete profile page\n",
    "custom_stat_selects = [\n",
    "    \"li:nth-child({})\",\n",
    "    \"> .stats-section:nth-child({}) \",\n",
    "    \"> table > tbody > tr:nth-child({}) > td\"\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### special mentions on scoring\n",
    "There are several different types of \"traditional\" workout styles in CrossFit: for time, AMRAP, 1RM, and so on. During the CrossFit Open, most workouts have a time cap. For example, in a workout involving 100 reps of *X* with a 10 minute time cap (this is a *for time* workout), if at 10 minutes the athlete has only finished 50 reps, then their score is recorded as 50 reps. If, however, the athlete finishes the workout under the time cap, let's say in 7 minutes, then the athlete's score is 7 minutes.\n",
    "\n",
    "Because of this, the Open leaderboard often has *for time* workout columns with 2 different score formats: xyz reps for athletes who did not finish, OR ab:cd:ef time for those that did. I didn't think about this during the first pass in data collection, so recorded scores were messed up, and I'm now redoing the scraping process with this obstacle in sight.\n",
    "\n",
    "To handle this scoring so that I can keep the same database schema (note it could be done by changing the database setup, but it may complicate the machine learning process), the following scoring will be done standard is enforced:\n",
    "* athletes who finish a workout in ab:cd:ef time will have their score recorded as follows\n",
    "    * (ab) * 3600 + (cd) * 60 + (ef) seconds\n",
    "* athletes who DO NOT finish a workout under a time cap gh:ij:kl, but who completed uvw out of xyz reps, will have their score recorded as follows\n",
    "    * (gh) * 3600 + (ij) * 60 + (kl) + (xyz - uvw) seconds\n",
    "    \n",
    "This scoring system penalizes athletes who did not finish a workout by 1 second for each missed rep. Although this scoring system may not seem fair to those who actually finished the workout, it has 2 benefits:\n",
    "* it allows all workout data to be consolidated into 1 value\n",
    "* it's easy to go back and extract their actual scores if a different transformation is needed later"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#for time workouts (note how 18.2 and 18.2a are considered 18.2)\n",
    "for_time_wods = [\"18.2\", \"18.3\", \"18.4\"]\n",
    "for_time_indices = [4, 6, 7]\n",
    "\n",
    "#18.2\n",
    "#https://games-assets.crossfit.com/18_2_15_aosi89035aiwSDOFIHhawe.pdf\n",
    "#18.3\n",
    "#https://games-assets.crossfit.com/2018-3_11-sbwyuet661293bse-ewyh.pdf\n",
    "#18.4\n",
    "#https://games-assets.crossfit.com/Workout18_4_12-shdnuehqASQbdsuE32w.pdf\n",
    "for_time_caps = [12*60, 14*60, 9*60]\n",
    "for_time_reps = [110, 928, 165]\n",
    "\n",
    "for_time_map = {}\n",
    "for i in range(len(for_time_indices)):\n",
    "    for_time_map[for_time_indices[i]] = {\n",
    "        \"time_cap\": for_time_caps[i],\n",
    "        \"needed_reps\": for_time_reps[i]\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{4: {'needed_reps': 110, 'time_cap': 720},\n",
       " 6: {'needed_reps': 928, 'time_cap': 840},\n",
       " 7: {'needed_reps': 165, 'time_cap': 540}}"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "for_time_map"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reformatting data\n",
    "The below funciton is used to reformat a lb/kg, reps, or hours:minutes:seconds value into it's database-equivalent format."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def convert_time_string_to_secs_int(string):\n",
    "    \"\"\"\n",
    "    Converts a string of the form \"ab:cd:ef\" to it's time equivalent in seconds\n",
    "    by doing int(ab) * 3600 + int(cd) * 60 + int(ef).\n",
    "    \"\"\"\n",
    "    hrs_mins_secs = list(map(lambda x: int(x), string.split(\":\")))\n",
    "    len_diff = len(time_mults) - len(hrs_mins_secs)\n",
    "    #collect all seconds\n",
    "    return sum(\n",
    "        [hrs_mins_secs[i] * time_mults[i + len_diff]\n",
    "             for i in range(len(hrs_mins_secs))]\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def handle_crossfit_score(html):\n",
    "    \"\"\"\n",
    "    Handles the parsing of several different CrossFit scores such as\n",
    "    weight (lb), time (x:y:z), reps (reps)/(), or none (--). Returns the\n",
    "    vale parsed to it's database required equivalent.\n",
    "    \"\"\"\n",
    "    #reps\n",
    "    if html[-4:] == \"reps\":\n",
    "        return int(html[:-5])\n",
    "    #time\n",
    "    elif \":\" in html:\n",
    "        return convert_time_string_to_secs_int(html)\n",
    "    #no score\n",
    "    elif html == \"--\":\n",
    "        return -1\n",
    "    #weight\n",
    "    elif html[-2:] == \"lb\" or html[-2:] == \"kg\":\n",
    "        weight = int(html[:-3])\n",
    "        return weight if html[-2:] == \"lb\" else int(round(weight * 2.2))\n",
    "    #reps with no reps on the end (used for max pull-ups in athlete profile)\n",
    "    else:\n",
    "        #handle special values with -2 entry (200 reps for pullups is not happening in 2018)\n",
    "        parsed_val = int(html)\n",
    "        return parsed_val if parsed_val < 150 else -2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Random\n",
    "The below code has been refactored several times. However, the current implementation forces the divisions to be scraped at random. Once a division's leaderboard has been completely collected, the division is removed as a *required* scraping target (deleted from the local copy of division pages). This means that on iteration 17, the next 50 athletes from the Men's division could be scraped, and on iteration 18 Girl's (14-15) could be the target."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from random import randint"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The second codeblock here is the legacy version of the first codeblock. The legacy version completely scrapes a single division until it's completely collected, and then moves onto the next. The newer version randomly samples from the remaining incompletely scraped division on each iteration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                   Division Women (60+) 5/45\n",
      "Page 5\n",
      "                                         Division Men 5/4552\n",
      "Page 5\n",
      "                                Division Women (55-59) 11/76\n",
      "Page 11\n",
      "                                 Division Boys (14-15) 13/40\n",
      "Page 13\n",
      "                                Division Girls (14-15) 10/32\n",
      "Page 10\n",
      "                                         Division Men 6/4552\n",
      "Page 6\n",
      "                                 Division Boys (14-15) 14/40\n",
      "Page 14\n",
      "                                 Division Girls (16-17) 4/47\n",
      "Page 4\n",
      "                                   Division Men (55-59) 4/91\n",
      "Page 4\n",
      "                                  Division Men (45-49) 9/337\n",
      "Page 9\n",
      "                                 Division Girls (16-17) 5/47\n",
      "Page 5\n",
      "                                Division Women (50-54) 5/124\n",
      "Page 5\n",
      "                                Division Women (45-49) 6/239\n",
      "Page 6\n",
      "                                         Division Men 7/4552\n",
      "Page 7\n",
      "                                 Division Boys (14-15) 15/40\n",
      "Page 15\n",
      "                                   Division Women (60+) 6/45\n",
      "Page 6\n",
      "                                Division Women (35-39) 3/619\n",
      "Page 3\n",
      "                                 Division Men (45-49) 10/337\n",
      "Page 10\n",
      "                                Division Women (45-49) 7/239\n",
      "Page 7\n",
      "                                  Division Men (35-39) 9/894\n",
      "Page 9\n",
      "                                  Division Men (50-54) 6/167\n",
      "Page 6\n",
      "                                 Division Boys (14-15) 16/40\n",
      "Page 16\n",
      "                                 Division Girls (16-17) 6/47\n",
      "Page 6\n",
      "                                Division Women (35-39) 4/619\n",
      "Page 4\n",
      "                                Division Girls (14-15) 11/32\n",
      "Page 11\n",
      "                                 Division Girls (16-17) 7/47\n",
      "Page 7\n",
      "                                 Division Girls (16-17) 8/47\n",
      "Page 8\n",
      "                                Division Women (35-39) 5/619\n",
      "Page 5\n",
      "                                Division Women (35-39) 6/619\n",
      "Page 6\n",
      "                                         Division Men 8/4552\n",
      "Page 8\n",
      "                                Division Women (35-39) 7/619\n",
      "Page 7\n",
      "                                Division Women (35-39) 8/619\n",
      "Page 8\n",
      "                                   Division Women (60+) 7/45\n",
      "Page 7\n",
      "                                 Division Boys (14-15) 17/40\n",
      "Page 17\n",
      "                                  Division Boys (16-17) 4/63\n",
      "Page 4\n",
      "                                 Division Men (40-44) 12/568\n",
      "Page 12\n",
      "                                Division Women (50-54) 6/124\n",
      "Page 6\n",
      "                                  Division Men (50-54) 7/167\n",
      "Page 7\n",
      "                                  Division Men (50-54) 8/167\n",
      "Page 8\n",
      "                                Division Women (50-54) 7/124\n",
      "Page 7\n",
      "                                Division Women (50-54) 8/124\n",
      "Page 8\n",
      "                                 Division Men (40-44) 13/568\n",
      "Page 13\n",
      "                                 Division Girls (16-17) 9/47\n",
      "Page 9\n",
      "                                     Division Men (60+) 7/55\n",
      "Page 7\n",
      "                                     Division Men (60+) 8/55\n",
      "Page 8\n",
      "                                Division Women (40-44) 6/398\n",
      "Page 6\n",
      "                                  Division Boys (16-17) 5/63\n",
      "Page 5\n",
      "                                 Division Men (40-44) 14/568\n",
      "Page 14\n",
      "                                Division Women (55-59) 12/76\n",
      "Page 12\n",
      "                                Division Girls (16-17) 10/47\n",
      "Page 10\n",
      "                                Division Women (45-49) 8/239\n",
      "Page 8\n",
      "                                  Division Boys (16-17) 6/63\n",
      "Page 6\n",
      "                                Division Women (40-44) 7/398\n",
      "Page 7\n",
      "                                Division Women (40-44) 8/398\n",
      "Page 8\n",
      "                                 Division Men (40-44) 15/568\n",
      "Page 15\n",
      "                                Division Girls (16-17) 11/47\n",
      "Page 11\n",
      "                                Division Women (35-39) 9/619\n",
      "Page 9\n",
      "                                  Division Men (50-54) 9/167\n",
      "Page 9\n",
      "                                 Division Men (40-44) 16/568\n",
      "Page 16\n",
      "                                 Division Boys (14-15) 18/40\n",
      "Page 18\n",
      "                                       Division Women 6/3440\n",
      "Page 6\n",
      "                                         Division Men 9/4552\n",
      "Page 9\n",
      "                                Division Girls (16-17) 12/47\n",
      "Page 12\n",
      "                                   Division Men (55-59) 5/91\n",
      "Page 5\n",
      "                                 Division Men (45-49) 11/337\n",
      "Page 11\n",
      "                                        Division Men 10/4552\n",
      "Page 10\n",
      "                                 Division Men (40-44) 17/568\n",
      "Page 17\n",
      "                                 Division Men (40-44) 18/568\n",
      "Page 18\n",
      "                                Division Girls (14-15) 12/32\n",
      "Page 12\n",
      "                                Division Women (50-54) 9/124\n",
      "Page 9\n",
      "                                 Division Men (50-54) 10/167\n",
      "Page 10\n",
      "                                       Division Women 7/3440\n",
      "Page 7\n",
      "                                Division Women (40-44) 9/398\n",
      "Page 9\n",
      "                                  Division Boys (16-17) 7/63\n",
      "Page 7\n",
      "                                   Division Women (60+) 8/45\n",
      "Page 8\n",
      "                                 Division Men (50-54) 11/167\n",
      "Page 11\n",
      "                                  Division Boys (16-17) 8/63\n",
      "Page 8\n",
      "                                 Division Men (50-54) 12/167\n",
      "Page 12\n",
      "                                 Division Men (40-44) 19/568\n",
      "Page 19\n",
      "                                 Division Men (40-44) 20/568\n",
      "Page 20\n",
      "                                   Division Women (60+) 9/45\n",
      "Page 9\n",
      "                                 Division Men (50-54) 13/167\n",
      "Page 13\n",
      "                                     Division Men (60+) 9/55\n",
      "Page 9\n",
      "                                  Division Women (60+) 10/45\n",
      "Page 10\n",
      "                                 Division Men (45-49) 12/337\n",
      "Page 12\n",
      "                                Division Women (45-49) 9/239\n",
      "Page 9\n",
      "                               Division Women (45-49) 10/239\n",
      "Page 10\n",
      "                                 Division Men (40-44) 21/568\n",
      "Page 21\n",
      "                               Division Women (50-54) 10/124\n",
      "Page 10\n",
      "                                       Division Women 8/3440\n",
      "Page 8\n",
      "                               Division Women (40-44) 10/398\n",
      "Page 10\n",
      "                               Division Women (45-49) 11/239\n",
      "Page 11\n",
      "                                       Division Women 9/3440\n",
      "Page 9\n",
      "                                 Division Men (50-54) 14/167\n",
      "Page 14\n",
      "                                 Division Men (50-54) 15/167\n",
      "Page 15\n",
      "                                        Division Men 11/4552\n",
      "Page 11\n",
      "                                  Division Boys (16-17) 9/63\n",
      "Page 9\n",
      "                                      Division Women 10/3440\n",
      "Page 10\n",
      "                               Division Women (35-39) 10/619\n",
      "Page 10\n",
      "                               Division Women (35-39) 11/619\n",
      "Page 11\n",
      "                                 Division Boys (14-15) 19/40\n",
      "Page 19\n",
      "                                 Division Boys (14-15) 20/40\n",
      "Page 20\n",
      "                                Division Women (55-59) 13/76\n",
      "Page 13\n",
      "                                 Division Boys (14-15) 21/40\n",
      "Page 21\n",
      "                                      Division Women 11/3440\n",
      "Page 11\n",
      "                                 Division Men (35-39) 10/894\n",
      "Page 10\n",
      "                                        Division Men 12/4552\n",
      "Page 12\n",
      "                               Division Women (40-44) 11/398\n",
      "Page 11\n",
      "                               Division Women (50-54) 11/124\n",
      "Page 11\n",
      "                                 Division Men (45-49) 13/337\n",
      "Page 13\n",
      "                                Division Girls (14-15) 13/32\n",
      "Page 13\n",
      "                                Division Women (55-59) 14/76\n",
      "Page 14\n",
      "                                      Division Women 12/3440\n",
      "Page 12\n",
      "                                  Division Women (60+) 11/45\n",
      "Page 11\n",
      "                               Division Women (35-39) 12/619\n",
      "Page 12\n",
      "                                Division Girls (14-15) 14/32\n",
      "Page 14\n",
      "                                 Division Men (40-44) 22/568\n",
      "Page 22\n",
      "                                 Division Men (40-44) 23/568\n",
      "Page 23\n",
      "                                 Division Men (40-44) 24/568\n",
      "Page 24\n",
      "                               Division Women (35-39) 13/619\n",
      "Page 13\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                      Division Women 13/3440\n",
      "Page 13\n",
      "                                Division Girls (14-15) 15/32\n",
      "Page 15\n",
      "                                 Division Boys (14-15) 22/40\n",
      "Page 22\n",
      "                                 Division Men (50-54) 16/167\n",
      "Page 16\n",
      "                               Division Women (35-39) 14/619\n",
      "Page 14\n",
      "                                 Division Boys (14-15) 23/40\n",
      "Page 23\n",
      "                               Division Women (45-49) 12/239\n",
      "Page 12\n",
      "                               Division Women (45-49) 13/239\n",
      "Page 13\n",
      "                                Division Women (55-59) 15/76\n",
      "Page 15\n",
      "                                        Division Men 13/4552\n",
      "Page 13\n",
      "                                Division Women (55-59) 16/76\n",
      "Page 16\n",
      "                                 Division Men (45-49) 14/337\n",
      "Page 14\n",
      "                                 Division Men (50-54) 17/167\n",
      "Page 17\n",
      "                                  Division Women (60+) 12/45\n",
      "Page 12\n",
      "                               Division Women (45-49) 14/239\n",
      "Page 14\n",
      "                               Division Women (50-54) 12/124\n",
      "Page 12\n",
      "                               Division Women (35-39) 15/619\n",
      "Page 15\n",
      "                                 Division Men (35-39) 11/894\n",
      "Page 11\n",
      "                                 Division Men (40-44) 25/568\n",
      "Page 25\n",
      "                                    Division Men (60+) 10/55\n",
      "Page 10\n",
      "                                Division Girls (16-17) 13/47\n",
      "Page 13\n",
      "                                 Division Men (50-54) 18/167\n",
      "Page 18\n",
      "                                    Division Men (60+) 11/55\n",
      "Page 11\n",
      "                               Division Women (35-39) 16/619\n",
      "Page 16\n",
      "                                  Division Women (60+) 13/45\n",
      "Page 13\n",
      "                                  Division Women (60+) 14/45\n",
      "Page 14\n",
      "                                 Division Boys (14-15) 24/40\n",
      "Page 24\n",
      "                                 Division Men (50-54) 19/167\n",
      "Page 19\n",
      "                                 Division Men (40-44) 26/568\n",
      "Page 26\n",
      "                                   Division Men (55-59) 6/91\n",
      "Page 6\n",
      "                                  Division Women (60+) 15/45\n",
      "Page 15\n",
      "                                  Division Women (60+) 16/45\n",
      "Page 16\n",
      "                                   Division Men (55-59) 7/91\n",
      "Page 7\n",
      "                                 Division Men (40-44) 27/568\n",
      "Page 27\n",
      "                               Division Women (45-49) 15/239\n",
      "Page 15\n",
      "                               Division Women (50-54) 13/124\n",
      "Page 13\n",
      "                                      Division Women 14/3440\n",
      "Page 14\n",
      "                               Division Women (40-44) 12/398\n",
      "Page 12\n",
      "                                Division Women (55-59) 17/76\n",
      "Page 17\n",
      "                               Division Women (35-39) 17/619\n",
      "Page 17\n",
      "                                 Division Men (45-49) 15/337\n",
      "Page 15\n",
      "                               Division Women (35-39) 18/619\n",
      "Page 18\n",
      "                                  Division Women (60+) 17/45\n",
      "Page 17\n",
      "                                    Division Men (60+) 12/55\n",
      "Page 12\n",
      "                                 Division Boys (16-17) 10/63\n",
      "Page 10\n",
      "                                      Division Women 15/3440\n",
      "Page 15\n",
      "                                 Division Men (40-44) 28/568\n",
      "Page 28\n",
      "                                Division Girls (14-15) 16/32\n",
      "Page 16\n",
      "                                 Division Men (50-54) 20/167\n",
      "Page 20\n",
      "                                  Division Women (60+) 18/45\n",
      "Page 18\n",
      "                                Division Girls (16-17) 14/47\n",
      "Page 14\n",
      "                                      Division Women 16/3440\n",
      "Page 16\n",
      "                                 Division Men (45-49) 16/337\n",
      "Page 16\n",
      "                                Division Women (55-59) 18/76\n",
      "Page 18\n",
      "                                        Division Men 14/4552\n",
      "Page 14\n",
      "                                 Division Boys (16-17) 11/63\n",
      "Page 11\n",
      "                               Division Women (45-49) 16/239\n",
      "Page 16\n",
      "                                Division Girls (16-17) 15/47\n",
      "Page 15\n",
      "                                 Division Men (45-49) 17/337\n",
      "Page 17\n",
      "                                 Division Boys (14-15) 25/40\n",
      "Page 25\n",
      "                               Division Women (45-49) 17/239\n",
      "Page 17\n",
      "                                 Division Men (35-39) 12/894\n",
      "Page 12\n",
      "                               Division Women (45-49) 18/239\n",
      "Page 18\n",
      "                               Division Women (50-54) 14/124\n",
      "Page 14\n",
      "                               Division Women (35-39) 19/619\n",
      "Page 19\n",
      "                                  Division Women (60+) 19/45\n",
      "Page 19\n",
      "                               Division Women (50-54) 15/124\n",
      "Page 15\n",
      "                               Division Women (50-54) 16/124\n",
      "Page 16\n",
      "                               Division Women (35-39) 20/619\n",
      "Page 20\n",
      "                                        Division Men 15/4552\n",
      "Page 15\n",
      "                               Division Women (45-49) 19/239\n",
      "Page 19\n",
      "                                  Division Women (60+) 20/45\n",
      "Page 20\n",
      "                               Division Women (35-39) 21/619\n",
      "Page 21\n",
      "                                 Division Men (35-39) 13/894\n",
      "Page 13\n",
      "                                 Division Boys (14-15) 26/40\n",
      "Page 26\n",
      "                                      Division Women 17/3440\n",
      "Page 17\n",
      "                                 Division Men (40-44) 29/568\n",
      "Page 29\n"
     ]
    },
    {
     "ename": "WebDriverException",
     "evalue": "Message: chrome not reachable\n  (Session info: chrome=65.0.3325.181)\n  (Driver info: chromedriver=2.35.528161 (5b82f2d2aae0ca24b877009200ced9065a772e73),platform=Windows NT 10.0.16299 x86_64)\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mWebDriverException\u001b[0m                        Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-23-2439c6ddb47d>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m    103\u001b[0m     \u001b[1;31m#iterate over athletes\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    104\u001b[0m     \u001b[1;32mfor\u001b[0m \u001b[0ma\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mathletes\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 105\u001b[1;33m         \u001b[0mdriver\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mathlete_url\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0ma\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"id\"\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    106\u001b[0m         \u001b[1;31m#wait for bottom stats to laod\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    107\u001b[0m         \u001b[1;31m#get leaderboard (might have to wait)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\conda\\conda\\envs\\crossfit_open_2018\\lib\\site-packages\\selenium\\webdriver\\remote\\webdriver.py\u001b[0m in \u001b[0;36mget\u001b[1;34m(self, url)\u001b[0m\n\u001b[0;32m    322\u001b[0m         \u001b[0mLoads\u001b[0m \u001b[0ma\u001b[0m \u001b[0mweb\u001b[0m \u001b[0mpage\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mcurrent\u001b[0m \u001b[0mbrowser\u001b[0m \u001b[0msession\u001b[0m\u001b[1;33m.\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    323\u001b[0m         \"\"\"\n\u001b[1;32m--> 324\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mexecute\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mCommand\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mGET\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m{\u001b[0m\u001b[1;34m'url'\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0murl\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    325\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    326\u001b[0m     \u001b[1;33m@\u001b[0m\u001b[0mproperty\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\conda\\conda\\envs\\crossfit_open_2018\\lib\\site-packages\\selenium\\webdriver\\remote\\webdriver.py\u001b[0m in \u001b[0;36mexecute\u001b[1;34m(self, driver_command, params)\u001b[0m\n\u001b[0;32m    310\u001b[0m         \u001b[0mresponse\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcommand_executor\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mexecute\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdriver_command\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mparams\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    311\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mresponse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 312\u001b[1;33m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0merror_handler\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcheck_response\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mresponse\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    313\u001b[0m             response['value'] = self._unwrap_value(\n\u001b[0;32m    314\u001b[0m                 response.get('value', None))\n",
      "\u001b[1;32m~\\AppData\\Local\\conda\\conda\\envs\\crossfit_open_2018\\lib\\site-packages\\selenium\\webdriver\\remote\\errorhandler.py\u001b[0m in \u001b[0;36mcheck_response\u001b[1;34m(self, response)\u001b[0m\n\u001b[0;32m    240\u001b[0m                 \u001b[0malert_text\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mvalue\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'alert'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'text'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    241\u001b[0m             \u001b[1;32mraise\u001b[0m \u001b[0mexception_class\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmessage\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mscreen\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstacktrace\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0malert_text\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 242\u001b[1;33m         \u001b[1;32mraise\u001b[0m \u001b[0mexception_class\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmessage\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mscreen\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstacktrace\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    243\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    244\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_value_or_default\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mobj\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkey\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdefault\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mWebDriverException\u001b[0m: Message: chrome not reachable\n  (Session info: chrome=65.0.3325.181)\n  (Driver info: chromedriver=2.35.528161 (5b82f2d2aae0ca24b877009200ced9065a772e73),platform=Windows NT 10.0.16299 x86_64)\n"
     ]
    }
   ],
   "source": [
    "#spinup driver\n",
    "driver = webdriver.Chrome()\n",
    "\n",
    "#for each division\n",
    "#for div in division_pages:\n",
    "while 0 < len(filtered_division_pages):\n",
    "    #select division randomly\n",
    "    div = filtered_division_pages[randint(0, len(filtered_division_pages) - 1)]\n",
    "    \n",
    "    #output division\n",
    "    print(\"Division {} {}/{}\".format(filter_maps[\"division\"][div[0]], div[1], div[2])\n",
    "             .rjust(center_space, center_sep))\n",
    "    print(\"Page {}\".format(div[1]))\n",
    "\n",
    "    #go to page\n",
    "    driver.get(custom_url.format(div[0], div[1]))\n",
    "\n",
    "    #get leaderboard (might have to wait)\n",
    "    lb = WebDriverWait(driver, 5).until(\n",
    "        EC.presence_of_element_located(\n",
    "            (\n",
    "                By.CSS_SELECTOR,\n",
    "                \"body > #containerOverlay > #leaderboard > .lb-main > .inner > table > tbody\"\n",
    "            )\n",
    "        )\n",
    "    )\n",
    "    #get rows\n",
    "    rows = lb.find_elements_by_xpath(\"*\")\n",
    "    #collect athlete data\n",
    "    athletes = []\n",
    "    for r in rows:\n",
    "        #get columns and create empty dictionary\n",
    "        cols = r.find_elements_by_xpath(\"*\")#only td elements\n",
    "        dic = {}\n",
    "\n",
    "        #column 1\n",
    "        #name\n",
    "        names = cols[1].find_elements_by_css_selector(\"div > div > div:nth-child(2) > div\")\n",
    "        dic[name_key] = (\n",
    "            \"'\" + (\n",
    "                names[0].get_attribute(\"innerText\") + \" \" + names[1].get_attribute(\"innerText\")\n",
    "            ).replace(\"'\", \"\\\\'\") + \"'\"\n",
    "        )\n",
    "        #info = cols[1].find_element_by_class_name(\"info\")\n",
    "        info = cols[1].find_element_by_css_selector(\"div > .bottom > ul\")\n",
    "        info_lis = info.find_elements_by_css_selector(\"li\")\n",
    "        #id (profile identifier for url)\n",
    "        dic[id_key] = int(info.find_element_by_tag_name(\"a\").get_attribute(\"href\").split(\"/\")[-1])\n",
    "        #region/division ids\n",
    "        dic[region_key] = filter_maps[\"region\"][info_lis[0].get_attribute(\"innerText\")]\n",
    "        dic[division_key] = div[0]\n",
    "        #age\n",
    "        dic[age_key] = int(info_lis[1].get_attribute(\"innerText\").split(\" \")[1])\n",
    "        #height/weight (not mandatory fields)\n",
    "        if 2 < len(info_lis):\n",
    "            height_weight = info_lis[2].get_attribute(\"innerText\").split(\" \")\n",
    "            #height\n",
    "            if \"in\" in height_weight:\n",
    "                dic[height_key] = float(height_weight[height_weight.index(\"in\") - 1])\n",
    "            elif \"cm\" in height_weight:\n",
    "                dic[height_key] = float(height_weight[height_weight.index(\"cm\") - 1]) / 2.54\n",
    "            else:\n",
    "                dic[height_key] = -1\n",
    "            dic[height_key] = int(round(dic[height_key]))\n",
    "            #weight\n",
    "            if \"lb\" in height_weight:\n",
    "                dic[weight_key] = float(height_weight[height_weight.index(\"lb\") - 1])\n",
    "            elif \"kg\" in height_weight:\n",
    "                dic[weight_key] = float(height_weight[height_weight.index(\"kg\") - 1]) * 2.20462\n",
    "            else:\n",
    "                dic[weight_key] = -1\n",
    "            dic[weight_key] = int(round(dic[weight_key]))\n",
    "\n",
    "        #columns 3-8 (inclusive, 0-indexed)\n",
    "        #leaderboard workout reps, time, or weight\n",
    "        scaled = False\n",
    "        first_col = 3\n",
    "        for i in range(first_col, 9):\n",
    "            #get inner value minus the surrounding parentheses\n",
    "            html_raw = cols[i].find_element_by_css_selector(\n",
    "                \"div > div > span > span:nth-child(2)\"\n",
    "            ).get_attribute(\"innerText\")\n",
    "            html = html_raw[html_raw.index(\"(\") + 1:-1]\n",
    "            #print(html_raw, html)\n",
    "            #if athlete scaled a workout, don't any data for them\n",
    "            if html.endswith(\"- s\"):\n",
    "                scaled = True\n",
    "                break\n",
    "            #convert values\n",
    "            key = workout_keys[i - first_col]\n",
    "            #handle html\n",
    "            dic[key] = handle_crossfit_score(html)\n",
    "            \n",
    "            #see markdown block for this explanation\n",
    "            #handle DNF (did not finish athletes for \"for time\" workouts 18.2 (not 18.2a), 18.3, 18.4\n",
    "            if i in for_time_map and html.endswith(\"reps\"):\n",
    "                #adjust score for penalty\n",
    "                dic[key] = for_time_map[i][\"time_cap\"] + for_time_map[i][\"needed_reps\"] - dic[key]\n",
    "\n",
    "        #append athlete if not scaled\n",
    "        if not scaled:\n",
    "            athletes.append(dic)\n",
    "    #iterate over athletes\n",
    "    for a in athletes:\n",
    "        driver.get(athlete_url.format(a[\"id\"]))\n",
    "        #wait for bottom stats to laod\n",
    "        #get leaderboard (might have to wait)\n",
    "        try:\n",
    "            #get stats\n",
    "            stats = WebDriverWait(driver, 1).until(\n",
    "                EC.presence_of_element_located(\n",
    "                    (\n",
    "                        By.CSS_SELECTOR,\n",
    "                        \"body #athleteProfile > div:nth-last-child(3) > .container > .stats-container\"\n",
    "                    )\n",
    "                )\n",
    "            )\n",
    "            #iterate and collect numbers\n",
    "            for i in range(1, 3):\n",
    "                sel0 = custom_stat_selects[0].format(i)\n",
    "                for j in range(1, 3):\n",
    "                    sel1 = custom_stat_selects[1].format(j)\n",
    "                    for k in range(1, 4):\n",
    "                        sel2 = custom_stat_selects[2].format(k)\n",
    "                        #select\n",
    "                        html = stats.find_element_by_css_selector(sel0 + sel1 + sel2).get_attribute(\"innerText\")\n",
    "                        a[stats_keys[6 * (i - 1) + 3 * (j - 1) + (k - 1)]] = handle_crossfit_score(html)\n",
    "        except:\n",
    "            #pass\n",
    "            for k in stats_keys:\n",
    "                a[k] = -1\n",
    "\n",
    "        #get affiliate id\n",
    "        try:\n",
    "            a[affiliate_key] = int(\n",
    "                (\n",
    "                    driver.find_element_by_css_selector(\n",
    "                        \"body #athleteProfile > .page-cover .bg-games-black-overlay .infobar > li:nth-child(6) a\"\n",
    "                    ).get_attribute(\"href\")\n",
    "                ).split(\"/\")[-1]\n",
    "            )\n",
    "        except:\n",
    "            a[affiliate_key] = -1\n",
    "\n",
    "    #\"stats-container\" class may not be present on profile pages (bs, dl, cj, ...)\n",
    "    #for a in athletes:\n",
    "    #    print(\"\\n\".join([\"{}: {}\".format(k, a[k]) for k in sorted(list(a.keys()))]) + \"\\n\")\n",
    "\n",
    "    #update database\n",
    "    try:\n",
    "        con = get_connect()\n",
    "        with con.cursor() as cur:\n",
    "            #insert athletes (if any from this page did not scale)\n",
    "            if len(athletes) != 0:\n",
    "                sql = \"\"\"\n",
    "                INSERT INTO athlete ({}) VALUES\n",
    "                    {}\n",
    "                    ON DUPLICATE KEY UPDATE id={};\n",
    "                \"\"\".format(\n",
    "                    all_keys_str,\n",
    "                    \",\\n\".join([\"(\" + \",\".join([str(a[k]) for k in all_keys]) + \")\" for a in athletes]),\n",
    "                    id_key\n",
    "                )\n",
    "                cur.execute(sql)\n",
    "                con.commit()\n",
    "\n",
    "            #increase value of current page\n",
    "            sql = \"\"\"\n",
    "            UPDATE worldwide_division_pages SET curr_page = {} WHERE division_id = {};\n",
    "            \"\"\".format(\n",
    "                div[1] + 1,\n",
    "                div[0]\n",
    "            )\n",
    "            cur.execute(sql)\n",
    "            con.commit()\n",
    "    finally:\n",
    "        if con:\n",
    "            con.close()\n",
    "\n",
    "    #increase local value of current page\n",
    "    div[1] += 1\n",
    "    #remove division if current page exceeds last page (after increment)\n",
    "    if div[1] == div[2] + 1:\n",
    "        filtered_division_pages.remove(div)\n",
    "\n",
    "#close browser\n",
    "driver.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "scrolled": true
   },
   "source": [
    "#spinup driver\n",
    "driver = webdriver.Chrome()\n",
    "\n",
    "#for each division\n",
    "for div in division_pages:\n",
    "    #output division\n",
    "    print(\"Division {}\".format(filter_maps[\"division\"][div[0]])\n",
    "             .rjust(center_space, center_sep))\n",
    "    #continue until current page exceeds last page\n",
    "    while div[1] <= div[2]:\n",
    "        print(\"Page {}\".format(div[1]))\n",
    "        \n",
    "        #go to page\n",
    "        driver.get(custom_url.format(div[0], div[1]))\n",
    "        \n",
    "        #get leaderboard (might have to wait)\n",
    "        lb = WebDriverWait(driver, 5).until(\n",
    "            EC.presence_of_element_located(\n",
    "                (\n",
    "                    By.CSS_SELECTOR,\n",
    "                    \"body > #containerOverlay > #leaderboard > .lb-main > .inner > table > tbody\"\n",
    "                )\n",
    "            )\n",
    "        )\n",
    "        #get rows\n",
    "        rows = lb.find_elements_by_xpath(\"*\")\n",
    "        #collect athlete data\n",
    "        athletes = []\n",
    "        for r in rows:\n",
    "            #get columns and create empty dictionary\n",
    "            cols = r.find_elements_by_xpath(\"*\")#only td elements\n",
    "            dic = {}\n",
    "            \n",
    "            #column 1\n",
    "            #name\n",
    "            names = cols[1].find_elements_by_css_selector(\"div > div > div:nth-child(2) > div\")\n",
    "            dic[name_key] = (\n",
    "                \"'\" + (\n",
    "                    names[0].get_attribute(\"innerText\") + \" \" + names[1].get_attribute(\"innerText\")\n",
    "                ).replace(\"'\", \"\\\\'\") + \"'\"\n",
    "            )\n",
    "            #info = cols[1].find_element_by_class_name(\"info\")\n",
    "            info = cols[1].find_element_by_css_selector(\"div > .bottom > ul\")\n",
    "            info_lis = info.find_elements_by_css_selector(\"li\")\n",
    "            #id (profile identifier for url)\n",
    "            dic[id_key] = int(info.find_element_by_tag_name(\"a\").get_attribute(\"href\").split(\"/\")[-1])\n",
    "            #region/division ids\n",
    "            dic[region_key] = filter_maps[\"region\"][info_lis[0].get_attribute(\"innerText\")]\n",
    "            dic[division_key] = div[0]\n",
    "            #age\n",
    "            dic[age_key] = int(info_lis[1].get_attribute(\"innerText\").split(\" \")[1])\n",
    "            #height/weight (not mandatory fields)\n",
    "            if 2 < len(info_lis):\n",
    "                height_weight = info_lis[2].get_attribute(\"innerText\").split(\" \")\n",
    "                #height\n",
    "                if \"in\" in height_weight:\n",
    "                    dic[height_key] = float(height_weight[height_weight.index(\"in\") - 1])\n",
    "                elif \"cm\" in height_weight:\n",
    "                    dic[height_key] = float(height_weight[height_weight.index(\"cm\") - 1]) / 2.54\n",
    "                else:\n",
    "                    dic[height_key] = -1\n",
    "                dic[height_key] = int(round(dic[height_key]))\n",
    "                #weight\n",
    "                if \"lb\" in height_weight:\n",
    "                    dic[weight_key] = float(height_weight[height_weight.index(\"lb\") - 1])\n",
    "                elif \"kg\" in height_weight:\n",
    "                    dic[weight_key] = float(height_weight[height_weight.index(\"kg\") - 1]) * 2.20462\n",
    "                else:\n",
    "                    dic[weight_key] = -1\n",
    "                dic[weight_key] = int(round(dic[weight_key]))\n",
    "            \n",
    "            #columns 3-8 (inclusive, 0-indexed)\n",
    "            #leaderboard workout reps, time, or weight\n",
    "            scaled = False\n",
    "            first_col = 3\n",
    "            for i in range(first_col, 9):\n",
    "                #get inner value minus the surrounding parentheses\n",
    "                html_raw = cols[i].find_element_by_css_selector(\n",
    "                    \"div > div > span > span:nth-child(2)\"\n",
    "                ).get_attribute(\"innerText\")\n",
    "                html = html_raw[html_raw.index(\"(\") + 1:-1]\n",
    "                #print(html_raw, html)\n",
    "                #if athlete scaled a workout, don't any data\n",
    "                if html.endswith(\"- s\"):\n",
    "                    scaled = True\n",
    "                    break\n",
    "                #convert values\n",
    "                key = workout_keys[i - first_col]\n",
    "                #handle html\n",
    "                dic[key] = handle_crossfit_score(html)\n",
    "\n",
    "            #append athlete if not scaled\n",
    "            if not scaled:\n",
    "                athletes.append(dic)\n",
    "        #iterate over athletes\n",
    "        for a in athletes:\n",
    "            driver.get(athlete_url.format(a[\"id\"]))\n",
    "            #wait for bottom stats to laod\n",
    "            #get leaderboard (might have to wait)\n",
    "            try:\n",
    "                #get stats\n",
    "                stats = WebDriverWait(driver, 1).until(\n",
    "                    EC.presence_of_element_located(\n",
    "                        (\n",
    "                            By.CSS_SELECTOR,\n",
    "                            \"body #athleteProfile > div:nth-last-child(3) > .container > .stats-container\"\n",
    "                        )\n",
    "                    )\n",
    "                )\n",
    "                #iterate and collect numbers\n",
    "                for i in range(1, 3):\n",
    "                    sel0 = custom_stat_selects[0].format(i)\n",
    "                    for j in range(1, 3):\n",
    "                        sel1 = custom_stat_selects[1].format(j)\n",
    "                        for k in range(1, 4):\n",
    "                            sel2 = custom_stat_selects[2].format(k)\n",
    "                            #select\n",
    "                            html = stats.find_element_by_css_selector(sel0 + sel1 + sel2).get_attribute(\"innerText\")\n",
    "                            a[stats_keys[6 * (i - 1) + 3 * (j - 1) + (k - 1)]] = handle_crossfit_score(html)\n",
    "            except:\n",
    "                #pass\n",
    "                for k in stats_keys:\n",
    "                    a[k] = -1\n",
    "            \n",
    "            #get affiliate id\n",
    "            try:\n",
    "                a[affiliate_key] = int(\n",
    "                    (\n",
    "                        driver.find_element_by_css_selector(\n",
    "                            \"body #athleteProfile > .page-cover .bg-games-black-overlay .infobar > li:nth-child(6) a\"\n",
    "                        ).get_attribute(\"href\")\n",
    "                    ).split(\"/\")[-1]\n",
    "                )\n",
    "            except:\n",
    "                a[affiliate_key] = -1\n",
    "            \n",
    "        #\"stats-container\" class may not be present on profile pages (bs, dl, cj, ...)\n",
    "        #for a in athletes:\n",
    "        #    print(\"\\n\".join([\"{}: {}\".format(k, a[k]) for k in sorted(list(a.keys()))]) + \"\\n\")\n",
    "        \n",
    "        #update database\n",
    "        try:\n",
    "            con = get_connect()\n",
    "            with con.cursor() as cur:\n",
    "                #insert athletes (if any from this page did not scale)\n",
    "                if len(athletes) != 0:\n",
    "                    sql = \"\"\"\n",
    "                    INSERT INTO athlete ({}) VALUES {};\n",
    "                    \"\"\".format(\n",
    "                        all_keys_str,\n",
    "                        \",\\n\".join([\"(\" + \",\".join([str(a[k]) for k in all_keys]) + \")\" for a in athletes])\n",
    "                    )\n",
    "                    cur.execute(sql)\n",
    "                    con.commit()\n",
    "                    \n",
    "                #increase value of current page\n",
    "                sql = \"\"\"\n",
    "                UPDATE worldwide_division_pages SET curr_page = {} WHERE division_id = {};\n",
    "                \"\"\".format(\n",
    "                    div[1] + 1,\n",
    "                    div[0]\n",
    "                )\n",
    "                cur.execute(sql)\n",
    "                con.commit()\n",
    "        finally:\n",
    "            if con:\n",
    "                con.close()\n",
    "        \n",
    "        #increase local value of current page\n",
    "        div[1] += 1\n",
    "\n",
    "#close browser\n",
    "driver.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
